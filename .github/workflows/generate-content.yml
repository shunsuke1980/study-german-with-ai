# ãƒ•ã‚¡ã‚¤ãƒ«: .github/workflows/generate-content.yml
name: Generate Daily German Content

on:
  schedule:
    # æ¯æ—¥åˆå‰9æ™‚ï¼ˆUTCï¼‰ã«å®Ÿè¡Œ = æ—¥æœ¬æ™‚é–“18æ™‚ã€ãƒ‰ã‚¤ãƒ„æ™‚é–“10æ™‚/11æ™‚
    - cron: '0 9 * * *'
  workflow_dispatch: # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½

jobs:
  generate-content:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install anthropic requests

    # ã“ã“ã‹ã‚‰é‡è¦ï¼šPythonã‚³ãƒ¼ãƒ‰ãŒç›´æ¥åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã¾ã™
    - name: Generate German content with vocabulary management
      env:
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        python << 'EOF'
        import json
        import os
        import re
        from datetime import datetime, timedelta
        from pathlib import Path
        import anthropic

        # èªå½™ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆã“ã“ã«å‰å›ã®VocabularyManagerãŒåŸ‹ã‚è¾¼ã¾ã‚Œã‚‹ï¼‰
        class VocabularyManager:
            def __init__(self, data_dir="data/vocabulary"):
                self.data_dir = Path(data_dir)
                self.data_dir.mkdir(parents=True, exist_ok=True)

                self.used_words_file = self.data_dir / "used_words.json"
                self.word_frequency_file = self.data_dir / "word_frequency.json"
                self.topics_used_file = self.data_dir / "topics_used.json"
                self.difficulty_levels_file = self.data_dir / "difficulty_levels.json"
                self.a2_target_words_file = self.data_dir / "a2_target_words.json"

                self.load_data()
                self.initialize_a2_target_words()

            def initialize_a2_target_words(self):
                """A2ãƒ¬ãƒ™ãƒ«ã®ç›®æ¨™å˜èª1,500èªã‚’åˆæœŸåŒ–"""
                if not self.a2_target_words_file.exists():
                    # A2ãƒ¬ãƒ™ãƒ«ã®åŸºæœ¬èªå½™ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ãƒ‰ã‚¤ãƒ„èªA2èªå½™ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
                    a2_words = {
                        # å®¶æ—ãƒ»äººé–“é–¢ä¿‚ (80èª)
                        "family": ["familie", "mutter", "vater", "kind", "sohn", "tochter", "bruder", "schwester", "eltern", "groÃŸmutter", "groÃŸvater", "baby", "freund", "freundin", "nachbar", "kollege", "chef", "lehrer", "schÃ¼ler", "student"],

                        # èº«ä½“ãƒ»å¥åº· (70èª)
                        "body": ["kÃ¶rper", "kopf", "auge", "nase", "mund", "ohr", "hand", "finger", "arm", "bein", "fuÃŸ", "herz", "gesundheit", "krankheit", "schmerz", "fieber", "erkÃ¤ltung", "arzt", "medikament", "sport"],

                        # é£Ÿã¹ç‰©ãƒ»é£²ã¿ç‰© (120èª)
                        "food": ["essen", "trinken", "brot", "kÃ¤se", "fleisch", "fisch", "ei", "milch", "wasser", "kaffee", "tee", "bier", "wein", "apfel", "orange", "banane", "kartoffel", "reis", "nudeln", "salat", "suppe", "kuchen", "schokolade", "zucker", "salz", "pfeffer", "restaurant", "cafÃ©", "kÃ¼che", "kochen"],

                        # ä½å±…ãƒ»å®¶ (100èª)
                        "home": ["haus", "wohnung", "zimmer", "kÃ¼che", "badezimmer", "schlafzimmer", "wohnzimmer", "bett", "tisch", "stuhl", "sofa", "fenster", "tÃ¼r", "schlÃ¼ssel", "telefon", "computer", "fernseher", "radio", "lampe", "buch", "zeitung", "garten", "balkon"],

                        # æ™‚é–“ãƒ»æ—¥ç¨‹ (80èª)
                        "time": ["zeit", "tag", "woche", "monat", "jahr", "heute", "gestern", "morgen", "montag", "dienstag", "mittwoch", "donnerstag", "freitag", "samstag", "sonntag", "januar", "februar", "mÃ¤rz", "april", "mai", "juni", "juli", "august", "september", "oktober", "november", "dezember", "uhr", "minute", "stunde", "frÃ¼h", "spÃ¤t", "pÃ¼nktlich"],

                        # å¤©æ°—ãƒ»è‡ªç„¶ (60èª)
                        "weather": ["wetter", "sonne", "regen", "schnee", "wind", "warm", "kalt", "heiÃŸ", "kÃ¼hl", "wolke", "himmel", "berg", "see", "fluss", "baum", "blume", "tier", "hund", "katze", "vogel", "park", "wald"],

                        # äº¤é€šãƒ»æ—…è¡Œ (90èª)
                        "transport": ["auto", "bus", "zug", "flugzeug", "fahrrad", "straÃŸe", "bahnhof", "flughafen", "hotel", "reise", "urlaub", "koffer", "ticket", "fahren", "fliegen", "gehen", "laufen", "kommen", "ankommen", "abfahren", "links", "rechts", "geradeaus", "nah", "weit", "schnell", "langsam"],

                        # ä»•äº‹ãƒ»å­¦æ ¡ (100èª)
                        "work": ["arbeit", "beruf", "bÃ¼ro", "firma", "geld", "euro", "bezahlen", "kaufen", "verkaufen", "geschÃ¤ft", "markt", "bank", "post", "schule", "universitÃ¤t", "lernen", "studieren", "prÃ¼fung", "note", "buch", "heft", "stift", "computer", "internet", "e-mail"],

                        # æœè£…ãƒ»ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° (80èª)
                        "clothes": ["kleidung", "kleid", "hose", "hemd", "jacke", "mantel", "schuhe", "socken", "hut", "brille", "uhr", "ring", "tasche", "grÃ¶ÃŸe", "farbe", "rot", "blau", "grÃ¼n", "gelb", "schwarz", "weiÃŸ", "grau", "braun", "neu", "alt", "schÃ¶n", "hÃ¤sslich", "groÃŸ", "klein"],

                        # æ„Ÿæƒ…ãƒ»æ€§æ ¼ (70èª)
                        "emotions": ["liebe", "freude", "angst", "trauer", "wut", "glÃ¼ck", "stress", "mÃ¼de", "wach", "hungrig", "durstig", "satt", "zufrieden", "unzufrieden", "nervÃ¶s", "ruhig", "frÃ¶hlich", "traurig", "bÃ¶se", "nett", "freundlich", "hilfsbereit", "intelligent", "dumm", "fleiÃŸig", "faul"],

                        # åŸºæœ¬å‹•è©ãƒ»å½¢å®¹è© (200èª)
                        "basic": ["sein", "haben", "werden", "kÃ¶nnen", "mÃ¼ssen", "wollen", "sollen", "dÃ¼rfen", "mÃ¶gen", "geben", "nehmen", "machen", "tun", "sagen", "sprechen", "hÃ¶ren", "sehen", "verstehen", "wissen", "denken", "glauben", "finden", "suchen", "zeigen", "helfen", "arbeiten", "spielen", "schlafen", "essen", "trinken", "leben", "wohnen", "bleiben", "kommen", "gehen", "fahren", "lesen", "schreiben", "gut", "schlecht", "richtig", "falsch", "wichtig", "interessant", "langweilig", "einfach", "schwer", "mÃ¶glich", "unmÃ¶glich"],

                        # æ•°å­—ãƒ»é‡ (50èª)
                        "numbers": ["null", "eins", "zwei", "drei", "vier", "fÃ¼nf", "sechs", "sieben", "acht", "neun", "zehn", "elf", "zwÃ¶lf", "dreizehn", "vierzehn", "fÃ¼nfzehn", "sechzehn", "siebzehn", "achtzehn", "neunzehn", "zwanzig", "dreiÃŸig", "vierzig", "fÃ¼nfzig", "hundert", "tausend", "viel", "wenig", "alle", "nichts", "etwas"],

                        # ãã®ä»–æ—¥å¸¸èªå½™ (170èª)
                        "misc": ["ja", "nein", "bitte", "danke", "entschuldigung", "hallo", "tschÃ¼ss", "auf wiedersehen", "wie", "was", "wer", "wo", "wann", "warum", "welche", "hier", "dort", "da", "nur", "auch", "noch", "schon", "wieder", "immer", "nie", "oft", "manchmal", "vielleicht", "sicher", "natÃ¼rlich", "problem", "lÃ¶sung", "idee", "plan", "ziel", "wunsch", "hoffnung", "chance", "erfolg", "fehler", "grund", "beispiel", "frage", "antwort", "information", "nachricht", "brief", "paket", "geschenk", "party", "fest", "musik", "film", "foto", "spiel", "sport", "hobby", "interesse"]
                    }

                    # å…¨å˜èªã‚’ãƒ•ãƒ©ãƒƒãƒˆãªãƒªã‚¹ãƒˆã«å¤‰æ›
                    all_a2_words = []
                    for category, words in a2_words.items():
                        all_a2_words.extend(words)

                    # é‡è¤‡é™¤å»ã—ã¦1,500èªã«èª¿æ•´
                    unique_words = list(set(all_a2_words))[:1500]

                    target_data = {
                        "total_target_words": len(unique_words),
                        "words_by_category": a2_words,
                        "all_words": unique_words,
                        "words_learned": [],
                        "learning_progress": 0.0
                    }

                    self._save_json(target_data, self.a2_target_words_file)
                    self.a2_target_words = target_data
                else:
                    self.a2_target_words = self._load_json(self.a2_target_words_file, {})

            def load_data(self):
                """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
                self.used_words = self._load_json(self.used_words_file, {})
                self.word_frequency = self._load_json(self.word_frequency_file, {})
                self.topics_used = self._load_json(self.topics_used_file, [])
                self.difficulty_levels = self._load_json(self.difficulty_levels_file, {
                    "current_level": "B1",
                    "level_progression": [],
                    "grammar_patterns_used": []
                })

            def _load_json(self, file_path, default):
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        return json.load(f)
                return default

            def _save_json(self, data, file_path):
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

            def extract_words(self, text):
                """ãƒ‰ã‚¤ãƒ„èªå˜èªã‚’æŠ½å‡ºã—ã€ãƒ¬ãƒ™ãƒ«åˆ†é¡"""
                german_word_pattern = r'\b[a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ]+\b'
                words = re.findall(german_word_pattern, text.lower())

                # åŸºæœ¬çš„ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
                stop_words = {'der', 'die', 'das', 'und', 'oder', 'aber', 'ist', 'sind',
                             'war', 'waren', 'hat', 'haben', 'wird', 'werden', 'kann',
                             'kÃ¶nnte', 'ein', 'eine', 'einen', 'einem', 'einer', 'zu',
                             'von', 'mit', 'fÃ¼r', 'auf', 'in', 'an', 'bei', 'nach',
                             'vor', 'Ã¼ber', 'auch', 'nicht', 'nur', 'noch', 'wie', 'wenn'}

                filtered_words = [word for word in words if len(word) > 3 and word not in stop_words]
                return filtered_words

            def categorize_word_level(self, word):
                """å˜èªã®CEFRãƒ¬ãƒ™ãƒ«ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
                # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ãƒ‰ã‚¤ãƒ„èªèªå½™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
                if len(word) <= 5:
                    return "A1"
                elif len(word) <= 8:
                    return "A2"
                elif len(word) <= 12:
                    return "B1"
                else:
                    return "B2"

            def get_current_difficulty_level(self):
                """ç¾åœ¨ã®å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—ï¼ˆå¸¸ã«A2å›ºå®šï¼‰"""
                return "A2"

            def update_vocabulary(self, text, date_str):
                """èªå½™ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€A2é€²æ—ã‚’è¿½è·¡"""
                words = self.extract_words(text)

                if date_str not in self.used_words:
                    self.used_words[date_str] = []

                # A2ç›®æ¨™å˜èªã¨ã®ç…§åˆ
                new_a2_words_learned = []
                word_analysis = []

                for word in words:
                    level = self.categorize_word_level(word)
                    word_analysis.append({"word": word, "level": level})
                    self.word_frequency[word] = self.word_frequency.get(word, 0) + 1

                    # A2ç›®æ¨™èªå½™ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if word in self.a2_target_words["all_words"] and word not in self.a2_target_words["words_learned"]:
                        self.a2_target_words["words_learned"].append(word)
                        new_a2_words_learned.append(word)

                self.used_words[date_str] = word_analysis

                # é€²æ—ç‡æ›´æ–°
                self.a2_target_words["learning_progress"] = len(self.a2_target_words["words_learned"]) / self.a2_target_words["total_target_words"] * 100

                # é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«æ›´æ–°
                self.difficulty_levels["current_level"] = "A2"
                self.difficulty_levels["level_progression"].append({
                    "date": date_str,
                    "level": "A2",
                    "unique_words": len(set(words)),
                    "a2_words_learned": len(new_a2_words_learned),
                    "total_a2_progress": round(self.a2_target_words["learning_progress"], 2)
                })

                self.save_data()
                return words, new_a2_words_learned

            def get_recent_words(self, days=7):
                """æœ€è¿‘ä½¿ç”¨ã—ãŸå˜èªã‚’å–å¾—"""
                recent_words = set()
                today = datetime.now()

                for date_str, word_data in self.used_words.items():
                    try:
                        date = datetime.strptime(date_str, '%Y-%m-%d')
                        if (today - date).days <= days:
                            if isinstance(word_data, list) and len(word_data) > 0:
                                if isinstance(word_data[0], dict):
                                    recent_words.update([item["word"] for item in word_data])
                                else:
                                    recent_words.update(word_data)
                    except (ValueError, KeyError):
                        continue

                return recent_words

            def get_overused_words(self, threshold=3):
                """ä½¿ã„ã™ãã¦ã„ã‚‹å˜èªã‚’å–å¾—"""
                return {word: count for word, count in self.word_frequency.items()
                        if count >= threshold}

            def generate_avoid_list(self):
                """90-120æ—¥ã§1,500èªã‚«ãƒãƒ¼ã‚’ç›®æŒ‡ã™ã‚¹ãƒãƒ¼ãƒˆå›é¿ãƒªã‚¹ãƒˆ"""
                total_days = len(self.used_words)
                target_words_per_day = 1500 / 120  # ç´„12.5èª/æ—¥
                words_learned = len(self.a2_target_words["words_learned"])

                # é€²æ—ã«å¿œã˜ãŸå‹•çš„èª¿æ•´
                if total_days > 0:
                    current_pace = words_learned / total_days
                    target_pace = 1500 / 120

                    if current_pace < target_pace * 0.8:  # é€²æ—ãŒé…ã„å ´åˆ
                        # ã‚ˆã‚Šç©æ¥µçš„ã«æ–°èªå½™ã‚’å°å…¥ï¼ˆå›é¿ã‚’ç·©ã‚ã‚‹ï¼‰
                        recent_days = 3
                        frequency_threshold = 4
                    elif current_pace > target_pace * 1.2:  # é€²æ—ãŒæ—©ã„å ´åˆ
                        # å¾©ç¿’é‡è¦–ï¼ˆå›é¿ã‚’å³ã—ãï¼‰
                        recent_days = 14
                        frequency_threshold = 2
                    else:  # é©æ­£ãƒšãƒ¼ã‚¹
                        recent_days = 7
                        frequency_threshold = 3
                else:
                    recent_days = 7
                    frequency_threshold = 3

                # æ®µéšçš„å›é¿ãƒªã‚¹ãƒˆç”Ÿæˆ
                overused = set(self.get_overused_words(threshold=frequency_threshold).keys())
                recent = self.get_recent_words(days=recent_days)

                # æœªå­¦ç¿’ã®A2å˜èªã‚’å„ªå…ˆçš„ã«å«ã‚ã‚‹ã‚ˆã†èª¿æ•´
                unlearned_a2_words = set(self.a2_target_words["all_words"]) - set(self.a2_target_words["words_learned"])

                # å›é¿ãƒªã‚¹ãƒˆã‹ã‚‰æœªå­¦ç¿’A2å˜èªã‚’é™¤å¤–
                avoid_list = list((overused.union(recent)) - unlearned_a2_words)

                return avoid_list[:60]  # æœ€å¤§60èªã«åˆ¶é™ï¼ˆClaude APIã®åˆ¶é™è€ƒæ…®ï¼‰

            def get_unused_topics(self):
                """æœªä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯ã‚’å–å¾—ï¼ˆA2ãƒ¬ãƒ™ãƒ«ã®ã¿ï¼‰"""
                a2_topics = [
                    "Familie und Freunde", "Essen und Trinken", "Einkaufen und GeschÃ¤fte",
                    "Wohnen und Hausarbeit", "Tagesablauf und Routine", "Wetter und Jahreszeiten",
                    "Verkehrsmittel und Reisen", "Kleidung und Mode", "KÃ¶rper und Gesundheit",
                    "Schule und Bildung", "Arbeit und Beruf", "Hobbys und Freizeit",
                    "Stadt und Land", "Restaurants und CafÃ©s", "Sport und Bewegung",
                    "Feste und Feiertage", "Tiere und Natur", "Technologie im Alltag",
                    "Geld und Preise", "Zeit und Termine", "GefÃ¼hle und Emotionen",
                    "Nachbarn und Gemeinschaft", "Arztbesuch und Apotheke", "Ã–ffentliche Verkehrsmittel"
                ]

                # æœ€è¿‘ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯é™¤å¤–ï¼ˆæœŸé–“ã‚’çŸ­ç¸®ã—ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼‰
                recent_topics = set()
                today = datetime.now()
                for topic_entry in self.topics_used:
                    try:
                        date = datetime.strptime(topic_entry['date'], '%Y-%m-%d')
                        if (today - date).days <= 7:  # 1é€±é–“ã«çŸ­ç¸®
                            recent_topics.add(topic_entry['topic'])
                    except (ValueError, KeyError):
                        continue

                return [topic for topic in a2_topics if topic not in recent_topics]

            def add_used_topic(self, topic, date_str):
                """ä½¿ç”¨æ¸ˆã¿ãƒˆãƒ”ãƒƒã‚¯ã‚’è¿½åŠ """
                self.topics_used.append({
                    'topic': topic,
                    'date': date_str,
                    'level': "A2"  # å¸¸ã«A2å›ºå®š
                })
                self.save_data()

            def save_data(self):
                """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
                self._save_json(self.used_words, self.used_words_file)
                self._save_json(self.word_frequency, self.word_frequency_file)
                self._save_json(self.topics_used, self.topics_used_file)
                self._save_json(self.difficulty_levels, self.difficulty_levels_file)
                self._save_json(self.a2_target_words, self.a2_target_words_file)

            def generate_unique_title(self, topic):
                """é‡è¤‡ã—ãªã„ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
                # æ—¢å­˜ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                existing_titles = set()
                blog_dir = Path("blog")
                if blog_dir.exists():
                    for file_path in blog_dir.glob("*.md"):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                if content.startswith('---'):
                                    yaml_end = content.find('---', 3)
                                    if yaml_end != -1:
                                        yaml_content = content[3:yaml_end]
                                        for line in yaml_content.split('\n'):
                                            if line.strip().startswith('title:'):
                                                title = line.split('title:', 1)[1].strip().strip('"\'')
                                                existing_titles.add(title)
                        except:
                            continue

                # ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«
                base_title = f"[A2] {topic}"

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
                if base_title not in existing_titles:
                    return base_title

                # é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã€ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
                topic_variations = {
                    "Familie und Freunde": ["Familienleben", "Meine Familie", "Freundschaft", "Verwandte"],
                    "Essen und Trinken": ["Lieblingsessen", "Kochen", "Restaurant", "GetrÃ¤nke", "Mahlzeiten"],
                    "Einkaufen und GeschÃ¤fte": ["Shopping", "Supermarkt", "Kleiderkauf", "Markt"],
                    "Wohnen und Hausarbeit": ["Mein Zuhause", "Hausarbeit", "Wohnungssuche", "MÃ¶bel"],
                    "Tagesablauf und Routine": ["Mein Tag", "Morgendliche Routine", "Arbeitsalltag", "Wochenende"],
                    "Wetter und Jahreszeiten": ["Heute ist schÃ¶nes Wetter", "FrÃ¼hling", "Winter", "Sommer", "Herbst"],
                    "Verkehrsmittel und Reisen": ["Mit dem Bus fahren", "Urlaubsreise", "Bahnfahrt", "Flugzeug"],
                    "Kleidung und Mode": ["Was ziehe ich an?", "Neue Kleidung", "Modetrends", "Schuhe"],
                    "KÃ¶rper und Gesundheit": ["Beim Arzt", "Sport treiben", "Gesund leben", "Krankheit"],
                    "Schule und Bildung": ["In der Schule", "Deutschlernen", "PrÃ¼fungen", "UniversitÃ¤t"],
                    "Arbeit und Beruf": ["Mein Job", "Im BÃ¼ro", "Arbeitssuche", "Kollegen"],
                    "Hobbys und Freizeit": ["Meine Hobbys", "WochenendaktivitÃ¤ten", "Sport", "Musik hÃ¶ren"],
                    "Stadt und Land": ["Stadtleben", "Auf dem Land", "Meine Stadt", "Dorfbesuch"],
                    "Restaurants und CafÃ©s": ["Im Restaurant", "CafÃ©-Besuch", "Bestellung", "Rechnung zahlen"],
                    "Sport und Bewegung": ["Fitness", "FuÃŸball spielen", "Schwimmen", "Wandern"],
                    "Feste und Feiertage": ["Geburtstag feiern", "Weihnachten", "Ostern", "Hochzeit"],
                    "Tiere und Natur": ["Haustiere", "Im Zoo", "Waldspaziergang", "Blumen"],
                    "Technologie im Alltag": ["Handy benutzen", "Computer", "Internet", "Social Media"],
                    "Geld und Preise": ["Beim Einkaufen", "Bank", "Sparen", "Preise vergleichen"],
                    "Zeit und Termine": ["Terminplanung", "Uhrzeiten", "Kalender", "PÃ¼nktlichkeit"],
                    "GefÃ¼hle und Emotionen": ["Wie ich mich fÃ¼hle", "GlÃ¼cklich sein", "Traurige Momente", "Stress"],
                    "Nachbarn und Gemeinschaft": ["Nachbarschaft", "Freiwilligenarbeit", "Gemeinsam helfen", "StraÃŸenfest"],
                    "Arztbesuch und Apotheke": ["Termin beim Arzt", "In der Apotheke", "Medikamente", "Gesundheitscheck"],
                    "Ã–ffentliche Verkehrsmittel": ["U-Bahn fahren", "Busticket kaufen", "ZugverspÃ¤tung", "Fahrplan lesen"]
                }

                variations = topic_variations.get(topic, [f"Teil {i}" for i in range(1, 6)])

                for variation in variations:
                    candidate_title = f"[A2] {variation}"
                    if candidate_title not in existing_titles:
                        return candidate_title

                # ã™ã¹ã¦ä½¿ç”¨æ¸ˆã¿ã®å ´åˆã€æ—¥ä»˜ã‚’è¿½åŠ 
                today = datetime.now().strftime('%m-%d')
                return f"[A2] {topic} ({today})"

            def generate_daily_vocabulary_report(self):
                """æ¯æ—¥ã®èªå½™ä½¿ç”¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆA2é€²æ—å«ã‚€ï¼‰"""
                today = datetime.now()

                # å…¨ä½“çµ±è¨ˆ
                total_unique_words = len(self.word_frequency)
                total_words_used = sum(self.word_frequency.values())

                # A2é€²æ—çµ±è¨ˆ
                a2_words_learned = len(self.a2_target_words["words_learned"])
                a2_progress_percent = self.a2_target_words["learning_progress"]
                remaining_a2_words = self.a2_target_words["total_target_words"] - a2_words_learned

                # å­¦ç¿’ãƒšãƒ¼ã‚¹è¨ˆç®—
                total_days = len(self.used_words)
                if total_days > 0:
                    words_per_day = a2_words_learned / total_days
                    estimated_days_to_complete = remaining_a2_words / max(words_per_day, 1)
                else:
                    words_per_day = 0
                    estimated_days_to_complete = 0

                # æœ€ã‚‚ä½¿ç”¨é »åº¦ã®é«˜ã„å˜èªãƒˆãƒƒãƒ—10
                most_common = sorted(self.word_frequency.items(),
                                   key=lambda x: x[1], reverse=True)[:10]

                # æœ€è¿‘ã®èªå½™å¤šæ§˜æ€§ï¼ˆéå»7æ—¥é–“ï¼‰
                recent_unique_words = len(self.get_recent_words(days=7))

                # éåº¦ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å˜èª
                overused_words = self.get_overused_words(threshold=2)

                return {
                    "date": today.strftime('%Y-%m-%d'),
                    "total_unique_words": total_unique_words,
                    "total_words_used": total_words_used,
                    "recent_unique_words": recent_unique_words,
                    "most_common_words": most_common,
                    "overused_words": len(overused_words),
                    "vocabulary_diversity_score": round(total_unique_words / max(total_words_used, 1) * 100, 2),
                    "a2_progress": {
                        "words_learned": a2_words_learned,
                        "total_target": self.a2_target_words["total_target_words"],
                        "progress_percent": round(a2_progress_percent, 2),
                        "remaining_words": remaining_a2_words,
                        "daily_pace": round(words_per_day, 2),
                        "estimated_completion_days": round(estimated_days_to_complete, 0)
                    }
                }

        # ãƒ¡ã‚¤ãƒ³å‡¦ç†
        def main():
            print("ğŸš€ Starting German content generation...")

            # èªå½™ç®¡ç†ã®åˆæœŸåŒ–
            vocab_manager = VocabularyManager()

            # ä»Šæ—¥ã®æ—¥ä»˜
            today = datetime.now().strftime('%Y-%m-%d')
            print(f"ğŸ“… Generating content for: {today}")

            # ç¾åœ¨ã®å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—
            current_level = vocab_manager.difficulty_levels["current_level"]
            print(f"ğŸ¯ Current difficulty level: {current_level}")

            # å›é¿ã™ã¹ãå˜èªã¨ãƒˆãƒ”ãƒƒã‚¯é¸æŠ
            avoid_words = vocab_manager.generate_avoid_list()
            available_topics = vocab_manager.get_unused_topics()

            if not available_topics:
                # å…¨ãƒˆãƒ”ãƒƒã‚¯ä½¿ç”¨æ¸ˆã¿ã®å ´åˆã¯ãƒªã‚»ãƒƒãƒˆ
                print("â™»ï¸  Resetting topics - all topics used recently")
                vocab_manager.topics_used = []
                available_topics = vocab_manager.get_unused_topics()

            selected_topic = available_topics[0] if available_topics else "AlltÃ¤gliches Leben"
            print(f"ğŸ“ Selected topic: {selected_topic}")
            print(f"ğŸš« Avoiding {len(avoid_words)} recently used words")

            # ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´ï¼ˆA2å›ºå®šï¼‰
            level_instructions = "Verwende einfache SÃ¤tze und grundlegende A2-Vocabulary. Vermeide komplexe Grammatik. Fokussiere dich auf alltÃ¤gliche Situationen und praktische Vokabeln."

            # Claudeã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆ1,500èªA2ã‚«ãƒãƒ¬ãƒƒã‚¸é‡è¦–ï¼‰
            a2_progress = vocab_manager.a2_target_words["learning_progress"]
            words_learned = len(vocab_manager.a2_target_words["words_learned"])
            target_words = vocab_manager.a2_target_words["total_target_words"]

            prompt = f'''Schreibe einen interessanten deutschen Text zum Thema "{selected_topic}".

Anforderungen:
- Genau 300 WÃ¶rter
- Niveau: A2 (Grundlegendes Deutsch)
- {level_instructions}
- NatÃ¼rlicher, flieÃŸender Schreibstil
- ZIEL - Neue A2-Vokabeln einfÃ¼hren (Fortschritt: {words_learned}/{target_words} = {a2_progress:.1f}%)

WICHTIG fÃ¼r A2-Vokabular:
- Verwende grundlegende Alltagsvokabeln
- Fokussiere auf praktische, nÃ¼tzliche WÃ¶rter
- Vermeide zu einfache WÃ¶rter wie "der, die, das, und, ist"
- Bevorzuge Substantive, Verben und Adjektive des tÃ¤glichen Lebens

Vermeide diese bereits verwendeten WÃ¶rter:
{', '.join(avoid_words) if avoid_words else 'Keine spezifischen WÃ¶rter zu vermeiden'}

STRATEGISCHES ZIEL - Hilf beim Erreichen des A2-Vokabulars von 1.500 WÃ¶rtern in 120 Tagen.
Verwende fÃ¼r jedes Konzept verschiedene Ausdrucksweisen und Synonyme.

Der Text soll fÃ¼r A2-Deutschlernende interessant sein und viele neue praktische Vokabeln enthalten.
Schreibe nur den deutschen Text, keine zusÃ¤tzlichen ErklÃ¤rungen.'''

            print("ğŸ¤– Calling Claude API...")

            # Anthropic APIå‘¼ã³å‡ºã—
            client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])

            try:
                message = client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    messages=[{"role": "user", "content": prompt}]
                )

                generated_text = message.content[0].text.strip()
                print(f"âœ… Generated {len(generated_text.split())} words")

            except Exception as e:
                print(f"âŒ Error calling Claude API: {e}")
                return

            # èªå½™ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
            print("ğŸ“Š Updating vocabulary database...")
            new_words, new_a2_words = vocab_manager.update_vocabulary(generated_text, today)
            vocab_manager.add_used_topic(selected_topic, today)

            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
            unique_title = vocab_manager.generate_unique_title(selected_topic)

            # æ¯æ—¥ã®èªå½™çµ±è¨ˆç”Ÿæˆ
            print("ğŸ“Š Generating daily vocabulary statistics...")
            daily_report = vocab_manager.generate_daily_vocabulary_report()

            # ãƒ–ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            blog_dir = Path("blog")
            blog_dir.mkdir(exist_ok=True)

            # èªå½™çµ±è¨ˆã®è¨ˆç®—
            word_stats = {
                "total_words": len(generated_text.split()),
                "unique_new_words": len(set(new_words)),
                "new_a2_words": len(new_a2_words),
                "difficulty_level": "A2",
                "a2_progress": daily_report["a2_progress"]
            }

            blog_content = f"""---
title: "{unique_title}"
date: {today}
topic: "{selected_topic}"
difficulty_level: "A2"
word_count: {word_stats["total_words"]}
unique_words: {word_stats["unique_new_words"]}
new_a2_words: {word_stats["new_a2_words"]}
a2_progress_percent: {word_stats["a2_progress"]["progress_percent"]}
estimated_completion_days: {word_stats["a2_progress"]["estimated_completion_days"]}
generated: true
---

{generated_text}

---

**ğŸ“š A2-Lernfortschritt:**
- **Neue A2-WÃ¶rter heute**: {word_stats["new_a2_words"]}
- **A2-WÃ¶rter gelernt**: {word_stats["a2_progress"]["words_learned"]}/{word_stats["a2_progress"]["total_target"]} ({word_stats["a2_progress"]["progress_percent"]}%)
- **Verbleibende WÃ¶rter**: {word_stats["a2_progress"]["remaining_words"]}
- **Lerntempo**: {word_stats["a2_progress"]["daily_pace"]} WÃ¶rter/Tag
- **GeschÃ¤tzte Restzeit**: {word_stats["a2_progress"]["estimated_completion_days"]} Tage

**ğŸ“Š Tagesstatistiken:**
- Wortanzahl: {word_stats["total_words"]}
- Einzigartige neue WÃ¶rter: {word_stats["unique_new_words"]}
- Thema: {selected_topic}

**ğŸ¯ Neue A2-Vokabeln heute:**
{', '.join(new_a2_words) if new_a2_words else 'Keine neuen A2-ZielwÃ¶rter erkannt'}

---

**ğŸ“– Sprachhilfen / Language Support:**
- ğŸ‡¯ğŸ‡µ [æ—¥æœ¬èªè§£èª¬ / Japanese Explanation]({today}-jp.md)
- ğŸ‡ºğŸ‡¸ [English Explanation]({today}-en.md)
"""

            blog_file = blog_dir / f"{today}.md"
            with open(blog_file, 'w', encoding='utf-8') as f:
                f.write(blog_content)

            print(f"ğŸ“„ Main blog post created: {blog_file}")

            # æ—¥æœ¬èªè§£èª¬è¨˜äº‹ã®ç”Ÿæˆ
            print("ğŸ‡¯ğŸ‡µ Generating Japanese explanation...")

            japanese_explanation_prompt = f"""Create a detailed Japanese explanation article for A2 German learners based on this German text:

GERMAN TEXT:
{generated_text}

TOPIC: {selected_topic}
NEW A2 WORDS: {', '.join(new_a2_words)}

Please create a comprehensive Japanese explanation article that includes:

1. ğŸ¯ **ä»Šæ—¥ã®å­¦ç¿’ç›®æ¨™** - Learning objectives for the day
2. ğŸ“– **é‡è¦èªå½™è§£èª¬** - Detailed vocabulary explanation with:
   - Category grouping (å®¶æ—é–¢ä¿‚, è·æ¥­, æ€§æ ¼ãƒ»æ…‹åº¦, æ´»å‹• etc.)
   - Pronunciation in katakana
   - Example sentences
   - Grammar notes for each word
3. ğŸ“ **é‡è¦æ–‡æ³•ãƒã‚¤ãƒ³ãƒˆ** - Important grammar points from the text with:
   - Examples extracted from the original text
   - Detailed explanations of case changes, verb conjugations
   - Tables showing declensions where relevant
4. ğŸ—£ï¸ **å®Ÿç”¨ãƒ•ãƒ¬ãƒ¼ã‚º** - Practical phrases related to the topic

Format the article in markdown with proper headers, tables, and clear structure.
Focus on A2-level grammar and vocabulary that Japanese learners need.
Write everything in Japanese.
Do not include practice questions, study tips, or homework sections.
"""

            try:
                jp_message = client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=2000,
                    messages=[{"role": "user", "content": japanese_explanation_prompt}]
                )

                japanese_explanation = jp_message.content[0].text.strip()

                # æ—¥æœ¬èªè§£èª¬è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                jp_blog_content = f"""---
title: "{unique_title} - æ—¥æœ¬èªè§£èª¬"
date: {today}
topic: "{selected_topic}"
type: "japanese_explanation"
difficulty_level: "A2"
original_post: "{today}.md"
---

# ğŸ“š A2è§£èª¬: {selected_topic}

**åŸæ–‡è¨˜äº‹**: [{unique_title}]({today}.md)

---

{japanese_explanation}
"""

                jp_blog_file = blog_dir / f"{today}-jp.md"
                with open(jp_blog_file, 'w', encoding='utf-8') as f:
                    f.write(jp_blog_content)

                print(f"ğŸ“„ Japanese explanation created: {jp_blog_file}")

            except Exception as e:
                print(f"âŒ Error generating Japanese explanation: {e}")

            # è‹±èªè§£èª¬è¨˜äº‹ã®ç”Ÿæˆ
            print("ğŸ‡ºğŸ‡¸ Generating English explanation...")

            english_explanation_prompt = f"""Create a detailed English explanation article for A2 German learners based on this German text:

GERMAN TEXT:
{generated_text}

TOPIC: {selected_topic}
NEW A2 WORDS: {', '.join(new_a2_words)}

Please create a comprehensive English explanation article that includes:

1. ğŸ¯ **Today's Learning Goals** - Learning objectives for the day
2. ğŸ“– **Key Vocabulary Analysis** - Detailed vocabulary explanation with:
   - Category grouping (Family relationships, Professions, Character traits, Activities etc.)
   - Pronunciation guide in IPA or simple phonetics
   - Example sentences
   - Grammar notes and word formation
3. ğŸ“ **Important Grammar Points** - Important grammar points from the text with:
   - Examples extracted from the original text
   - Detailed explanations of case changes, verb conjugations
   - Tables showing declensions where relevant
4. ğŸ—£ï¸ **Useful Phrases** - Practical phrases related to the topic with translations

Format the article in markdown with proper headers, tables, and clear structure.
Focus on A2-level grammar and vocabulary explanations for English speakers learning German.
Write everything in clear, educational English.
Do not include practice exercises, study tips, or homework sections.
"""

            try:
                en_message = client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=2000,
                    messages=[{"role": "user", "content": english_explanation_prompt}]
                )

                english_explanation = en_message.content[0].text.strip()

                # è‹±èªè§£èª¬è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                en_blog_content = f"""---
title: "{unique_title} - English Explanation"
date: {today}
topic: "{selected_topic}"
type: "english_explanation"
difficulty_level: "A2"
original_post: "{today}.md"
---

# ğŸ“š A2 German Study Guide: {selected_topic}

**Original Article**: [{unique_title}]({today}.md)

---

{english_explanation}
"""

                en_blog_file = blog_dir / f"{today}-en.md"
                with open(en_blog_file, 'w', encoding='utf-8') as f:
                    f.write(en_blog_content)

                print(f"ğŸ“„ English explanation created: {en_blog_file}")

            except Exception as e:
                print(f"âŒ Error generating English explanation: {e}")
            print(f"ğŸ¯ Title: {unique_title}")
            print(f"ğŸ“ˆ New A2 words learned: {len(new_a2_words)}")
            print(f"ğŸ“š A2 Progress: {daily_report['a2_progress']['words_learned']}/{daily_report['a2_progress']['total_target']} ({daily_report['a2_progress']['progress_percent']}%)")
            print(f"â±ï¸  Estimated completion: {daily_report['a2_progress']['estimated_completion_days']} days")
            print(f"ğŸ‡¯ğŸ‡µ Japanese explanation: {today}-jp.md")
            print(f"ğŸ‡ºğŸ‡¸ English explanation: {today}-en.md")

            # A2é€²æ—ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´ææ¡ˆ
            if daily_report["a2_progress"]["estimated_completion_days"] > 150:
                print("âš ï¸  WARNING: A2 completion may take longer than 120 days! Increasing vocabulary focus.")
            elif daily_report["a2_progress"]["estimated_completion_days"] < 90:
                print("ğŸš€ EXCELLENT: On track to complete A2 vocabulary ahead of schedule!")
            else:
                print("âœ… GOOD: A2 vocabulary completion on target for ~120 days")

            # æ¯æ—¥ã®èªå½™ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            reports_dir = Path("data/reports")
            reports_dir.mkdir(parents=True, exist_ok=True)

            daily_report_file = reports_dir / f"{today}-vocabulary-report.json"
            with open(daily_report_file, 'w', encoding='utf-8') as f:
                json.dump(daily_report, f, ensure_ascii=False, indent=2)

            print(f"ğŸ“‹ Daily vocabulary report saved: {daily_report_file}")

            # é€±æ¬¡A2é€²æ—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ—¥æ›œæ—¥ã®ã¿ï¼‰
            if datetime.now().weekday() == 6:  # Sunday
                print("ğŸ“Š Generating weekly A2 progress summary...")

                # éå»7æ—¥é–“ã®A2å­¦ç¿’çµ±è¨ˆ
                week_a2_stats = {
                    "week_ending": today,
                    "a2_words_learned_total": daily_report["a2_progress"]["words_learned"],
                    "a2_progress_percent": daily_report["a2_progress"]["progress_percent"],
                    "estimated_completion": daily_report["a2_progress"]["estimated_completion_days"],
                    "daily_pace": daily_report["a2_progress"]["daily_pace"],
                    "remaining_words": daily_report["a2_progress"]["remaining_words"]
                }

                weekly_summary = f"""---
title: "[A2] WÃ¶chentlicher Fortschrittsbericht"
date: {today}
type: "weekly_progress_report"
---

# ğŸ“Š WÃ¶chentlicher A2-Deutschlern-Fortschritt

## ğŸ¯ A2-Vokabular Fortschritt
- **Gelernte A2-WÃ¶rter**: {week_a2_stats["a2_words_learned_total"]}/1.500 ({week_a2_stats["a2_progress_percent"]}%)
- **Verbleibende WÃ¶rter**: {week_a2_stats["remaining_words"]}
- **Aktuelles Lerntempo**: {week_a2_stats["daily_pace"]:.1f} neue A2-WÃ¶rter pro Tag
- **GeschÃ¤tzte Restzeit**: {week_a2_stats["estimated_completion"]} Tage

## ğŸ“ˆ Leistungsbeurteilung
{'ğŸš€ Ausgezeichnet! Vor dem Zeitplan.' if week_a2_stats["estimated_completion"] < 90 else 'âœ… Gut! Im Zeitplan fÃ¼r ~120 Tage.' if week_a2_stats["estimated_completion"] <= 130 else 'âš ï¸ Achtung: MÃ¶glicherweise lÃ¤nger als 120 Tage.'}

## ğŸª Empfehlungen
- Fokus auf alltÃ¤gliche Substantive und Verben
- Vermeidung bereits hÃ¤ufig verwendeter WÃ¶rter
- Gezieltes Lernen spezifischer A2-Themenbereiche
- RegelmÃ¤ÃŸige Wiederholung neuer Vokabeln
"""

                weekly_file = blog_dir / f"{today}-weekly-a2-progress.md"
                with open(weekly_file, 'w', encoding='utf-8') as f:
                    f.write(weekly_summary)
                print(f"ğŸ“‹ Weekly A2 progress report created: {weekly_file}")

        if __name__ == "__main__":
            main()
        EOF

    - name: Commit and push new content
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "German Content Generator"
        git add blog/ data/

        if git diff --staged --quiet; then
          echo "No new content to commit"
        else
          COMMIT_MSG="Add A2 German content $(date +%Y-%m-%d) - Focus on vocabulary diversity"
          if [ $(date +%u) -eq 7 ]; then
            COMMIT_MSG="$COMMIT_MSG + weekly vocabulary summary"
          fi
          git commit -m "$COMMIT_MSG [skip ci]"
          git push
        fi