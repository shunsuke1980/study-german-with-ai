#!/usr/bin/env python3
import json
import os
import re
import sys
from datetime import datetime, timedelta
from pathlib import Path

# Check API key first
if 'ANTHROPIC_API_KEY' not in os.environ:
    print("ERROR: ANTHROPIC_API_KEY not found in environment variables")
    sys.exit(1)

if not os.environ['ANTHROPIC_API_KEY']:
    print("ERROR: ANTHROPIC_API_KEY is empty")
    sys.exit(1)
    
print(f"API Key found (length: {len(os.environ['ANTHROPIC_API_KEY'])})")

try:
    import anthropic
    print("Successfully imported anthropic module")
except ImportError as e:
    print(f"ERROR: Failed to import anthropic: {e}")
    sys.exit(1)

# èªå½™ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆã“ã“ã«å‰å›ã®VocabularyManagerãŒåŸ‹ã‚è¾¼ã¾ã‚Œã‚‹ï¼‰
class VocabularyManager:
    def __init__(self, data_dir="data/vocabulary"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.used_words_file = self.data_dir / "used_words.json"
        self.word_frequency_file = self.data_dir / "word_frequency.json"
        self.topics_used_file = self.data_dir / "topics_used.json"
        self.difficulty_levels_file = self.data_dir / "difficulty_levels.json"
        self.a2_target_words_file = self.data_dir / "a2_target_words.json"

        self.load_data()
        self.initialize_a2_target_words()

    def initialize_a2_target_words(self):
        """A2ãƒ¬ãƒ™ãƒ«ã®ç›®æ¨™å˜èª1,500èªã‚’åˆæœŸåŒ–"""
        if not self.a2_target_words_file.exists():
            # A2ãƒ¬ãƒ™ãƒ«ã®åŸºæœ¬èªå½™ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ãƒ‰ã‚¤ãƒ„èªA2èªå½™ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
            a2_words = {
                # å®¶æ—ãƒ»äººé–“é–¢ä¿‚ (80èª)
                "family": ["familie", "mutter", "vater", "kind", "sohn", "tochter", "bruder", "schwester", "eltern", "groÃŸmutter", "groÃŸvater", "baby", "freund", "freundin", "nachbar", "kollege", "chef", "lehrer", "schÃ¼ler", "student"],

                # èº«ä½“ãƒ»å¥åº· (70èª)
                "body": ["kÃ¶rper", "kopf", "auge", "nase", "mund", "ohr", "hand", "finger", "arm", "bein", "fuÃŸ", "herz", "gesundheit", "krankheit", "schmerz", "fieber", "erkÃ¤ltung", "arzt", "medikament", "sport"],

                # é£Ÿã¹ç‰©ãƒ»é£²ã¿ç‰© (120èª)
                "food": ["essen", "trinken", "brot", "kÃ¤se", "fleisch", "fisch", "ei", "milch", "wasser", "kaffee", "tee", "bier", "wein", "apfel", "orange", "banane", "kartoffel", "reis", "nudeln", "salat", "suppe", "kuchen", "schokolade", "zucker", "salz", "pfeffer", "restaurant", "cafÃ©", "kÃ¼che", "kochen"],

                # ä½å±…ãƒ»å®¶ (100èª)
                "home": ["haus", "wohnung", "zimmer", "kÃ¼che", "badezimmer", "schlafzimmer", "wohnzimmer", "bett", "tisch", "stuhl", "sofa", "fenster", "tÃ¼r", "schlÃ¼ssel", "telefon", "computer", "fernseher", "radio", "lampe", "buch", "zeitung", "garten", "balkon"],

                # æ™‚é–“ãƒ»æ—¥ç¨‹ (80èª)
                "time": ["zeit", "tag", "woche", "monat", "jahr", "heute", "gestern", "morgen", "montag", "dienstag", "mittwoch", "donnerstag", "freitag", "samstag", "sonntag", "januar", "februar", "mÃ¤rz", "april", "mai", "juni", "juli", "august", "september", "oktober", "november", "dezember", "uhr", "minute", "stunde", "frÃ¼h", "spÃ¤t", "pÃ¼nktlich"],

                # å¤©æ°—ãƒ»è‡ªç„¶ (60èª)
                "weather": ["wetter", "sonne", "regen", "schnee", "wind", "warm", "kalt", "heiÃŸ", "kÃ¼hl", "wolke", "himmel", "berg", "see", "fluss", "baum", "blume", "tier", "hund", "katze", "vogel", "park", "wald"],

                # äº¤é€šãƒ»æ—…è¡Œ (90èª)
                "transport": ["auto", "bus", "zug", "flugzeug", "fahrrad", "straÃŸe", "bahnhof", "flughafen", "hotel", "reise", "urlaub", "koffer", "ticket", "fahren", "fliegen", "gehen", "laufen", "kommen", "ankommen", "abfahren", "links", "rechts", "geradeaus", "nah", "weit", "schnell", "langsam"],

                # ä»•äº‹ãƒ»å­¦æ ¡ (100èª)
                "work": ["arbeit", "beruf", "bÃ¼ro", "firma", "geld", "euro", "bezahlen", "kaufen", "verkaufen", "geschÃ¤ft", "markt", "bank", "post", "schule", "universitÃ¤t", "lernen", "studieren", "prÃ¼fung", "note", "buch", "heft", "stift", "computer", "internet", "e-mail"],

                # æœè£…ãƒ»ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° (80èª)
                "clothes": ["kleidung", "kleid", "hose", "hemd", "jacke", "mantel", "schuhe", "socken", "hut", "brille", "uhr", "ring", "tasche", "grÃ¶ÃŸe", "farbe", "rot", "blau", "grÃ¼n", "gelb", "schwarz", "weiÃŸ", "grau", "braun", "neu", "alt", "schÃ¶n", "hÃ¤sslich", "groÃŸ", "klein"],

                # æ„Ÿæƒ…ãƒ»æ€§æ ¼ (70èª)
                "emotions": ["liebe", "freude", "angst", "trauer", "wut", "glÃ¼ck", "stress", "mÃ¼de", "wach", "hungrig", "durstig", "satt", "zufrieden", "unzufrieden", "nervÃ¶s", "ruhig", "frÃ¶hlich", "traurig", "bÃ¶se", "nett", "freundlich", "hilfsbereit", "intelligent", "dumm", "fleiÃŸig", "faul"],

                # åŸºæœ¬å‹•è©ãƒ»å½¢å®¹è© (200èª)
                "basic": ["sein", "haben", "werden", "kÃ¶nnen", "mÃ¼ssen", "wollen", "sollen", "dÃ¼rfen", "mÃ¶gen", "geben", "nehmen", "machen", "tun", "sagen", "sprechen", "hÃ¶ren", "sehen", "verstehen", "wissen", "denken", "glauben", "finden", "suchen", "zeigen", "helfen", "arbeiten", "spielen", "schlafen", "essen", "trinken", "leben", "wohnen", "bleiben", "kommen", "gehen", "fahren", "lesen", "schreiben", "gut", "schlecht", "richtig", "falsch", "wichtig", "interessant", "langweilig", "einfach", "schwer", "mÃ¶glich", "unmÃ¶glich"],

                # æ•°å­—ãƒ»é‡ (50èª)
                "numbers": ["null", "eins", "zwei", "drei", "vier", "fÃ¼nf", "sechs", "sieben", "acht", "neun", "zehn", "elf", "zwÃ¶lf", "dreizehn", "vierzehn", "fÃ¼nfzehn", "sechzehn", "siebzehn", "achtzehn", "neunzehn", "zwanzig", "dreiÃŸig", "vierzig", "fÃ¼nfzig", "hundert", "tausend", "viel", "wenig", "alle", "nichts", "etwas"],

                # ãã®ä»–æ—¥å¸¸èªå½™ (170èª)
                "misc": ["ja", "nein", "bitte", "danke", "entschuldigung", "hallo", "tschÃ¼ss", "auf wiedersehen", "wie", "was", "wer", "wo", "wann", "warum", "welche", "hier", "dort", "da", "nur", "auch", "noch", "schon", "wieder", "immer", "nie", "oft", "manchmal", "vielleicht", "sicher", "natÃ¼rlich", "problem", "lÃ¶sung", "idee", "plan", "ziel", "wunsch", "hoffnung", "chance", "erfolg", "fehler", "grund", "beispiel", "frage", "antwort", "information", "nachricht", "brief", "paket", "geschenk", "party", "fest", "musik", "film", "foto", "spiel", "sport", "hobby", "interesse"]
            }

            # å…¨å˜èªã‚’ãƒ•ãƒ©ãƒƒãƒˆãªãƒªã‚¹ãƒˆã«å¤‰æ›
            all_a2_words = []
            for category, words in a2_words.items():
                all_a2_words.extend(words)

            # é‡è¤‡é™¤å»ã—ã¦1,500èªã«èª¿æ•´
            unique_words = list(set(all_a2_words))[:1500]

            target_data = {
                "total_target_words": len(unique_words),
                "words_by_category": a2_words,
                "all_words": unique_words,
                "words_learned": [],
                "learning_progress": 0.0
            }

            self._save_json(target_data, self.a2_target_words_file)
            self.a2_target_words = target_data
        else:
            self.a2_target_words = self._load_json(self.a2_target_words_file, {})

    def load_data(self):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        self.used_words = self._load_json(self.used_words_file, {})
        self.word_frequency = self._load_json(self.word_frequency_file, {})
        self.topics_used = self._load_json(self.topics_used_file, [])
        self.difficulty_levels = self._load_json(self.difficulty_levels_file, {
            "current_level": "B1",
            "level_progression": [],
            "grammar_patterns_used": []
        })

    def _load_json(self, file_path, default):
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return default

    def _save_json(self, data, file_path):
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def extract_words(self, text):
        """ãƒ‰ã‚¤ãƒ„èªå˜èªã‚’æŠ½å‡ºã—ã€ãƒ¬ãƒ™ãƒ«åˆ†é¡"""
        german_word_pattern = r'\b[a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ]+\b'
        words = re.findall(german_word_pattern, text.lower())

        # åŸºæœ¬çš„ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
        stop_words = {'der', 'die', 'das', 'und', 'oder', 'aber', 'ist', 'sind',
                     'war', 'waren', 'hat', 'haben', 'wird', 'werden', 'kann',
                     'kÃ¶nnte', 'ein', 'eine', 'einen', 'einem', 'einer', 'zu',
                     'von', 'mit', 'fÃ¼r', 'auf', 'in', 'an', 'bei', 'nach',
                     'vor', 'Ã¼ber', 'auch', 'nicht', 'nur', 'noch', 'wie', 'wenn'}

        filtered_words = [word for word in words if len(word) > 3 and word not in stop_words]
        return filtered_words

    def categorize_word_level(self, word):
        """å˜èªã®CEFRãƒ¬ãƒ™ãƒ«ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ãƒ‰ã‚¤ãƒ„èªèªå½™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
        if len(word) <= 5:
            return "A1"
        elif len(word) <= 8:
            return "A2"
        elif len(word) <= 12:
            return "B1"
        else:
            return "B2"

    def get_current_difficulty_level(self):
        """ç¾åœ¨ã®å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—ï¼ˆå¸¸ã«A2å›ºå®šï¼‰"""
        return "A2"

    def update_vocabulary(self, text, date_str):
        """èªå½™ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€A2é€²æ—ã‚’è¿½è·¡"""
        words = self.extract_words(text)

        if date_str not in self.used_words:
            self.used_words[date_str] = []

        # A2ç›®æ¨™å˜èªã¨ã®ç…§åˆ
        new_a2_words_learned = []
        word_analysis = []

        for word in words:
            level = self.categorize_word_level(word)
            word_analysis.append({"word": word, "level": level})
            self.word_frequency[word] = self.word_frequency.get(word, 0) + 1

            # A2ç›®æ¨™èªå½™ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if word in self.a2_target_words["all_words"] and word not in self.a2_target_words["words_learned"]:
                self.a2_target_words["words_learned"].append(word)
                new_a2_words_learned.append(word)

        self.used_words[date_str] = word_analysis

        # é€²æ—ç‡æ›´æ–°
        self.a2_target_words["learning_progress"] = len(self.a2_target_words["words_learned"]) / self.a2_target_words["total_target_words"] * 100

        # é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«æ›´æ–°
        self.difficulty_levels["current_level"] = "A2"
        self.difficulty_levels["level_progression"].append({
            "date": date_str,
            "level": "A2",
            "unique_words": len(set(words)),
            "a2_words_learned": len(new_a2_words_learned),
            "total_a2_progress": round(self.a2_target_words["learning_progress"], 2)
        })

        self.save_data()
        return words, new_a2_words_learned

    def get_recent_words(self, days=7):
        """æœ€è¿‘ä½¿ç”¨ã—ãŸå˜èªã‚’å–å¾—"""
        recent_words = set()
        today = datetime.now()

        for date_str, word_data in self.used_words.items():
            try:
                date = datetime.strptime(date_str, '%Y-%m-%d')
                if (today - date).days <= days:
                    if isinstance(word_data, list) and len(word_data) > 0:
                        if isinstance(word_data[0], dict):
                            recent_words.update([item["word"] for item in word_data])
                        else:
                            recent_words.update(word_data)
            except (ValueError, KeyError):
                continue

        return recent_words

    def get_overused_words(self, threshold=3):
        """ä½¿ã„ã™ãã¦ã„ã‚‹å˜èªã‚’å–å¾—"""
        return {word: count for word, count in self.word_frequency.items()
                if count >= threshold}

    def generate_avoid_list(self):
        """90-120æ—¥ã§1,500èªã‚«ãƒãƒ¼ã‚’ç›®æŒ‡ã™ã‚¹ãƒãƒ¼ãƒˆå›é¿ãƒªã‚¹ãƒˆ"""
        total_days = len(self.used_words)
        target_words_per_day = 1500 / 120  # ç´„12.5èª/æ—¥
        words_learned = len(self.a2_target_words["words_learned"])

        # é€²æ—ã«å¿œã˜ãŸå‹•çš„èª¿æ•´
        if total_days > 0:
            current_pace = words_learned / total_days
            target_pace = 1500 / 120

            if current_pace < target_pace * 0.8:  # é€²æ—ãŒé…ã„å ´åˆ
                # ã‚ˆã‚Šç©æ¥µçš„ã«æ–°èªå½™ã‚’å°å…¥ï¼ˆå›é¿ã‚’ç·©ã‚ã‚‹ï¼‰
                recent_days = 3
                frequency_threshold = 4
            elif current_pace > target_pace * 1.2:  # é€²æ—ãŒæ—©ã„å ´åˆ
                # å¾©ç¿’é‡è¦–ï¼ˆå›é¿ã‚’å³ã—ãï¼‰
                recent_days = 14
                frequency_threshold = 2
            else:  # é©æ­£ãƒšãƒ¼ã‚¹
                recent_days = 7
                frequency_threshold = 3
        else:
            recent_days = 7
            frequency_threshold = 3

        # æ®µéšçš„å›é¿ãƒªã‚¹ãƒˆç”Ÿæˆ
        overused = set(self.get_overused_words(threshold=frequency_threshold).keys())
        recent = self.get_recent_words(days=recent_days)

        # æœªå­¦ç¿’ã®A2å˜èªã‚’å„ªå…ˆçš„ã«å«ã‚ã‚‹ã‚ˆã†èª¿æ•´
        unlearned_a2_words = set(self.a2_target_words["all_words"]) - set(self.a2_target_words["words_learned"])

        # å›é¿ãƒªã‚¹ãƒˆã‹ã‚‰æœªå­¦ç¿’A2å˜èªã‚’é™¤å¤–
        avoid_list = list((overused.union(recent)) - unlearned_a2_words)

        return avoid_list[:60]  # æœ€å¤§60èªã«åˆ¶é™ï¼ˆClaude APIã®åˆ¶é™è€ƒæ…®ï¼‰

    def get_unused_topics(self):
        """æœªä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯ã‚’å–å¾—ï¼ˆA2ãƒ¬ãƒ™ãƒ«ã®ã¿ï¼‰"""
        a2_topics = [
            "Familie und Freunde", "Essen und Trinken", "Einkaufen und GeschÃ¤fte",
            "Wohnen und Hausarbeit", "Tagesablauf und Routine", "Wetter und Jahreszeiten",
            "Verkehrsmittel und Reisen", "Kleidung und Mode", "KÃ¶rper und Gesundheit",
            "Schule und Bildung", "Arbeit und Beruf", "Hobbys und Freizeit",
            "Stadt und Land", "Restaurants und CafÃ©s", "Sport und Bewegung",
            "Feste und Feiertage", "Tiere und Natur", "Technologie im Alltag",
            "Geld und Preise", "Zeit und Termine", "GefÃ¼hle und Emotionen",
            "Nachbarn und Gemeinschaft", "Arztbesuch und Apotheke", "Ã–ffentliche Verkehrsmittel"
        ]

        # æœ€è¿‘ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯é™¤å¤–ï¼ˆæœŸé–“ã‚’çŸ­ç¸®ã—ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼‰
        recent_topics = set()
        today = datetime.now()
        for topic_entry in self.topics_used:
            try:
                date = datetime.strptime(topic_entry['date'], '%Y-%m-%d')
                if (today - date).days <= 7:  # 1é€±é–“ã«çŸ­ç¸®
                    recent_topics.add(topic_entry['topic'])
            except (ValueError, KeyError):
                continue

        return [topic for topic in a2_topics if topic not in recent_topics]

    def add_used_topic(self, topic, date_str):
        """ä½¿ç”¨æ¸ˆã¿ãƒˆãƒ”ãƒƒã‚¯ã‚’è¿½åŠ """
        self.topics_used.append({
            'topic': topic,
            'date': date_str,
            'level': "A2"  # å¸¸ã«A2å›ºå®š
        })
        self.save_data()

    def save_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        self._save_json(self.used_words, self.used_words_file)
        self._save_json(self.word_frequency, self.word_frequency_file)
        self._save_json(self.topics_used, self.topics_used_file)
        self._save_json(self.difficulty_levels, self.difficulty_levels_file)
        self._save_json(self.a2_target_words, self.a2_target_words_file)

    def generate_unique_title(self, topic):
        """é‡è¤‡ã—ãªã„ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        # æ—¢å­˜ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        existing_titles = set()
        blog_dir = Path("blog")
        if blog_dir.exists():
            for file_path in blog_dir.glob("*.md"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if content.startswith('---'):
                            yaml_end = content.find('---', 3)
                            if yaml_end != -1:
                                yaml_content = content[3:yaml_end]
                                for line in yaml_content.split('\n'):
                                    if line.strip().startswith('title:'):
                                        title = line.split('title:', 1)[1].strip().strip('"\'')
                                        existing_titles.add(title)
                except:
                    continue

        # ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«
        base_title = f"[A2] {topic}"

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        if base_title not in existing_titles:
            return base_title

        # é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã€ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        topic_variations = {
            "Familie und Freunde": ["Familienleben", "Meine Familie", "Freundschaft", "Verwandte"],
            "Essen und Trinken": ["Lieblingsessen", "Kochen", "Restaurant", "GetrÃ¤nke", "Mahlzeiten"],
            "Einkaufen und GeschÃ¤fte": ["Shopping", "Supermarkt", "Kleiderkauf", "Markt"],
            "Wohnen und Hausarbeit": ["Mein Zuhause", "Hausarbeit", "Wohnungssuche", "MÃ¶bel"],
            "Tagesablauf und Routine": ["Mein Tag", "Morgendliche Routine", "Arbeitsalltag", "Wochenende"],
            "Wetter und Jahreszeiten": ["Heute ist schÃ¶nes Wetter", "FrÃ¼hling", "Winter", "Sommer", "Herbst"],
            "Verkehrsmittel und Reisen": ["Mit dem Bus fahren", "Urlaubsreise", "Bahnfahrt", "Flugzeug"],
            "Kleidung und Mode": ["Was ziehe ich an?", "Neue Kleidung", "Modetrends", "Schuhe"],
            "KÃ¶rper und Gesundheit": ["Beim Arzt", "Sport treiben", "Gesund leben", "Krankheit"],
            "Schule und Bildung": ["In der Schule", "Deutschlernen", "PrÃ¼fungen", "UniversitÃ¤t"],
            "Arbeit und Beruf": ["Mein Job", "Im BÃ¼ro", "Arbeitssuche", "Kollegen"],
            "Hobbys und Freizeit": ["Meine Hobbys", "WochenendaktivitÃ¤ten", "Sport", "Musik hÃ¶ren"],
            "Stadt und Land": ["Stadtleben", "Auf dem Land", "Meine Stadt", "Dorfbesuch"],
            "Restaurants und CafÃ©s": ["Im Restaurant", "CafÃ©-Besuch", "Bestellung", "Rechnung zahlen"],
            "Sport und Bewegung": ["Fitness", "FuÃŸball spielen", "Schwimmen", "Wandern"],
            "Feste und Feiertage": ["Geburtstag feiern", "Weihnachten", "Ostern", "Hochzeit"],
            "Tiere und Natur": ["Haustiere", "Im Zoo", "Waldspaziergang", "Blumen"],
            "Technologie im Alltag": ["Handy benutzen", "Computer", "Internet", "Social Media"],
            "Geld und Preise": ["Beim Einkaufen", "Bank", "Sparen", "Preise vergleichen"],
            "Zeit und Termine": ["Terminplanung", "Uhrzeiten", "Kalender", "PÃ¼nktlichkeit"],
            "GefÃ¼hle und Emotionen": ["Wie ich mich fÃ¼hle", "GlÃ¼cklich sein", "Traurige Momente", "Stress"],
            "Nachbarn und Gemeinschaft": ["Nachbarschaft", "Freiwilligenarbeit", "Gemeinsam helfen", "StraÃŸenfest"],
            "Arztbesuch und Apotheke": ["Termin beim Arzt", "In der Apotheke", "Medikamente", "Gesundheitscheck"],
            "Ã–ffentliche Verkehrsmittel": ["U-Bahn fahren", "Busticket kaufen", "ZugverspÃ¤tung", "Fahrplan lesen"]
        }

        variations = topic_variations.get(topic, [f"Teil {i}" for i in range(1, 6)])

        for variation in variations:
            candidate_title = f"[A2] {variation}"
            if candidate_title not in existing_titles:
                return candidate_title

        # ã™ã¹ã¦ä½¿ç”¨æ¸ˆã¿ã®å ´åˆã€æ—¥ä»˜ã‚’è¿½åŠ 
        today = datetime.now().strftime('%m-%d')
        return f"[A2] {topic} ({today})"

    def generate_daily_vocabulary_report(self):
        """æ¯æ—¥ã®èªå½™ä½¿ç”¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆA2é€²æ—å«ã‚€ï¼‰"""
        today = datetime.now()

        # å…¨ä½“çµ±è¨ˆ
        total_unique_words = len(self.word_frequency)
        total_words_used = sum(self.word_frequency.values())

        # A2é€²æ—çµ±è¨ˆ
        a2_words_learned = len(self.a2_target_words["words_learned"])
        a2_progress_percent = self.a2_target_words["learning_progress"]
        remaining_a2_words = self.a2_target_words["total_target_words"] - a2_words_learned

        # å­¦ç¿’ãƒšãƒ¼ã‚¹è¨ˆç®—
        total_days = len(self.used_words)
        if total_days > 0:
            words_per_day = a2_words_learned / total_days
            estimated_days_to_complete = remaining_a2_words / max(words_per_day, 1)
        else:
            words_per_day = 0
            estimated_days_to_complete = 0

        # æœ€ã‚‚ä½¿ç”¨é »åº¦ã®é«˜ã„å˜èªãƒˆãƒƒãƒ—10
        most_common = sorted(self.word_frequency.items(),
                           key=lambda x: x[1], reverse=True)[:10]

        # æœ€è¿‘ã®èªå½™å¤šæ§˜æ€§ï¼ˆéå»7æ—¥é–“ï¼‰
        recent_unique_words = len(self.get_recent_words(days=7))

        # éåº¦ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å˜èª
        overused_words = self.get_overused_words(threshold=2)

        return {
            "date": today.strftime('%Y-%m-%d'),
            "total_unique_words": total_unique_words,
            "total_words_used": total_words_used,
            "recent_unique_words": recent_unique_words,
            "most_common_words": most_common,
            "overused_words": len(overused_words),
            "vocabulary_diversity_score": round(total_unique_words / max(total_words_used, 1) * 100, 2),
            "a2_progress": {
                "words_learned": a2_words_learned,
                "total_target": self.a2_target_words["total_target_words"],
                "progress_percent": round(a2_progress_percent, 2),
                "remaining_words": remaining_a2_words,
                "daily_pace": round(words_per_day, 2),
                "estimated_completion_days": round(estimated_days_to_complete, 0)
            }
        }

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    print("ğŸš€ Starting German content generation...")

    # èªå½™ç®¡ç†ã®åˆæœŸåŒ–
    vocab_manager = VocabularyManager()

    # ä»Šæ—¥ã®æ—¥ä»˜
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"ğŸ“… Generating content for: {today}")

    # ç¾åœ¨ã®å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—
    current_level = vocab_manager.difficulty_levels["current_level"]
    print(f"ğŸ¯ Current difficulty level: {current_level}")

    # å›é¿ã™ã¹ãå˜èªã¨ãƒˆãƒ”ãƒƒã‚¯é¸æŠ
    avoid_words = vocab_manager.generate_avoid_list()
    available_topics = vocab_manager.get_unused_topics()

    if not available_topics:
        # å…¨ãƒˆãƒ”ãƒƒã‚¯ä½¿ç”¨æ¸ˆã¿ã®å ´åˆã¯ãƒªã‚»ãƒƒãƒˆ
        print("â™»ï¸  Resetting topics - all topics used recently")
        vocab_manager.topics_used = []
        available_topics = vocab_manager.get_unused_topics()

    selected_topic = available_topics[0] if available_topics else "AlltÃ¤gliches Leben"
    print(f"ğŸ“ Selected topic: {selected_topic}")
    print(f"ğŸš« Avoiding {len(avoid_words)} recently used words")

    # ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´ï¼ˆA2å›ºå®šï¼‰
    level_instructions = "Verwende einfache SÃ¤tze und grundlegende A2-Vocabulary. Vermeide komplexe Grammatik. Fokussiere dich auf alltÃ¤gliche Situationen und praktische Vokabeln."

    # Claudeã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆ1,500èªA2ã‚«ãƒãƒ¬ãƒƒã‚¸é‡è¦–ï¼‰
    a2_progress = vocab_manager.a2_target_words["learning_progress"]
    words_learned = len(vocab_manager.a2_target_words["words_learned"])
    target_words = vocab_manager.a2_target_words["total_target_words"]

    # Build avoid words string safely
    avoid_words_str = ", ".join(avoid_words) if avoid_words else "Keine spezifischen WÃ¶rter zu vermeiden"
    
    # Build prompt using string concatenation to avoid YAML issues
    prompt = "Schreibe einen interessanten deutschen Text zum Thema '" + selected_topic + "'.\n\n"
    prompt += "Anforderungen:\n"
    prompt += "- Genau 300 WÃ¶rter\n"
    prompt += "- Niveau A2 (Grundlegendes Deutsch)\n"
    prompt += "- " + level_instructions + "\n"
    prompt += "- NatÃ¼rlicher flieÃŸender Schreibstil\n"
    prompt += "- ZIEL Neue A2-Vokabeln einfÃ¼hren (Fortschritt " + str(words_learned) + "/" + str(target_words) + " entspricht " + str(round(a2_progress, 1)) + " Prozent)\n\n"
    prompt += "WICHTIG fÃ¼r A2-Vokabular:\n"
    prompt += "- Verwende grundlegende Alltagsvokabeln\n"
    prompt += "- Fokussiere auf praktische nÃ¼tzliche WÃ¶rter\n"
    prompt += "- Vermeide zu einfache WÃ¶rter wie der die das und ist\n"
    prompt += "- Bevorzuge Substantive Verben und Adjektive des tÃ¤glichen Lebens\n\n"
    prompt += "Vermeide diese bereits verwendeten WÃ¶rter:\n"
    prompt += avoid_words_str + "\n\n"
    prompt += "STRATEGISCHES ZIEL Hilf beim Erreichen des A2-Vokabulars von 1500 WÃ¶rtern in 120 Tagen.\n"
    prompt += "Verwende fÃ¼r jedes Konzept verschiedene Ausdrucksweisen und Synonyme.\n\n"
    prompt += "Der Text soll fÃ¼r A2-Deutschlernende interessant sein und viele neue praktische Vokabeln enthalten.\n"
    prompt += "Schreibe nur den deutschen Text keine zusÃ¤tzlichen ErklÃ¤rungen."

    print("ğŸ¤– Calling Claude API...")

    # Anthropic APIå‘¼ã³å‡ºã—
    client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])

    try:
        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )

        generated_text = message.content[0].text.strip()
        print(f"âœ… Generated {len(generated_text.split())} words")

    except Exception as e:
        print(f"âŒ Error calling Claude API: {e}")
        return

    # èªå½™ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
    print("ğŸ“Š Updating vocabulary database...")
    new_words, new_a2_words = vocab_manager.update_vocabulary(generated_text, today)
    vocab_manager.add_used_topic(selected_topic, today)

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
    unique_title = vocab_manager.generate_unique_title(selected_topic)

    # æ¯æ—¥ã®èªå½™çµ±è¨ˆç”Ÿæˆ
    print("ğŸ“Š Generating daily vocabulary statistics...")
    daily_report = vocab_manager.generate_daily_vocabulary_report()

    # ãƒ–ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    blog_dir = Path("blog")
    blog_dir.mkdir(exist_ok=True)

    # èªå½™çµ±è¨ˆã®è¨ˆç®—
    word_stats = {
        "total_words": len(generated_text.split()),
        "unique_new_words": len(set(new_words)),
        "new_a2_words": len(new_a2_words),
        "difficulty_level": "A2",
        "a2_progress": daily_report["a2_progress"]
    }

    # Build blog content using string concatenation
    new_a2_words_str = ', '.join(new_a2_words) if new_a2_words else 'Keine neuen A2-ZielwÃ¶rter erkannt'
    
    blog_content = "---\n"
    blog_content += 'title: "' + unique_title + '"\n'
    blog_content += "date: " + today + "\n"
    blog_content += 'topic: "' + selected_topic + '"\n'
    blog_content += 'difficulty_level: "A2"\n'
    blog_content += "word_count: " + str(word_stats["total_words"]) + "\n"
    blog_content += "unique_words: " + str(word_stats["unique_new_words"]) + "\n"
    blog_content += "new_a2_words: " + str(word_stats["new_a2_words"]) + "\n"
    blog_content += "a2_progress_percent: " + str(word_stats["a2_progress"]["progress_percent"]) + "\n"
    blog_content += "estimated_completion_days: " + str(word_stats["a2_progress"]["estimated_completion_days"]) + "\n"
    blog_content += "generated: true\n"
    blog_content += "---\n\n"
    blog_content += generated_text + "\n\n"
    blog_content += "---\n\n"
    blog_content += "**ğŸ“š A2-Lernfortschritt:**\n"
    blog_content += "- **Neue A2-WÃ¶rter heute**: " + str(word_stats["new_a2_words"]) + "\n"
    blog_content += "- **A2-WÃ¶rter gelernt**: " + str(word_stats["a2_progress"]["words_learned"]) + "/" + str(word_stats["a2_progress"]["total_target"]) + " (" + str(word_stats["a2_progress"]["progress_percent"]) + "%)\n"
    blog_content += "- **Verbleibende WÃ¶rter**: " + str(word_stats["a2_progress"]["remaining_words"]) + "\n"
    blog_content += "- **Lerntempo**: " + str(word_stats["a2_progress"]["daily_pace"]) + " WÃ¶rter/Tag\n"
    blog_content += "- **GeschÃ¤tzte Restzeit**: " + str(word_stats["a2_progress"]["estimated_completion_days"]) + " Tage\n\n"
    blog_content += "**ğŸ“Š Tagesstatistiken:**\n"
    blog_content += "- Wortanzahl: " + str(word_stats["total_words"]) + "\n"
    blog_content += "- Einzigartige neue WÃ¶rter: " + str(word_stats["unique_new_words"]) + "\n"
    blog_content += "- Thema: " + selected_topic + "\n\n"
    blog_content += "**ğŸ¯ Neue A2-Vokabeln heute:**\n"
    blog_content += new_a2_words_str + "\n\n"
    blog_content += "---\n\n"
    blog_content += "**ğŸ“– Sprachhilfen / Language Support:**\n"
    blog_content += "- ğŸ‡¯ğŸ‡µ [æ—¥æœ¬èªè§£èª¬ / Japanese Explanation](" + today + "-jp.md)\n"
    blog_content += "- ğŸ‡ºğŸ‡¸ [English Explanation](" + today + "-en.md)\n"

    blog_file = blog_dir / (today + ".md")
    with open(blog_file, 'w', encoding='utf-8') as f:
        f.write(blog_content)

    print("ğŸ“„ Main blog post created: " + str(blog_file))

    # æ—¥æœ¬èªè§£èª¬è¨˜äº‹ã®ç”Ÿæˆ
    print("ğŸ‡¯ğŸ‡µ Generating Japanese explanation...")

    # Build Japanese explanation prompt using string concatenation
    japanese_explanation_prompt = "Create a detailed Japanese explanation article for A2 German learners based on this German text:\n\n"
    japanese_explanation_prompt += "GERMAN TEXT:\n"
    japanese_explanation_prompt += generated_text + "\n\n"
    japanese_explanation_prompt += "TOPIC: " + selected_topic + "\n"
    japanese_explanation_prompt += "NEW A2 WORDS: " + ', '.join(new_a2_words) + "\n\n"
    japanese_explanation_prompt += "Please create a comprehensive Japanese explanation article that includes:\n\n"
    japanese_explanation_prompt += "1. ğŸ¯ **ä»Šæ—¥ã®å­¦ç¿’ç›®æ¨™** - Learning objectives for the day\n"
    japanese_explanation_prompt += "2. ğŸ“– **é‡è¦èªå½™è§£èª¬** - Detailed vocabulary explanation with:\n"
    japanese_explanation_prompt += "   - Category grouping (å®¶æ—é–¢ä¿‚, è·æ¥­, æ€§æ ¼ãƒ»æ…‹åº¦, æ´»å‹• etc.)\n"
    japanese_explanation_prompt += "   - Pronunciation in katakana\n"
    japanese_explanation_prompt += "   - Example sentences\n"
    japanese_explanation_prompt += "   - Grammar notes for each word\n"
    japanese_explanation_prompt += "3. ğŸ“ **é‡è¦æ–‡æ³•ãƒã‚¤ãƒ³ãƒˆ** - Important grammar points from the text with:\n"
    japanese_explanation_prompt += "   - Examples extracted from the original text\n"
    japanese_explanation_prompt += "   - Detailed explanations of case changes, verb conjugations\n"
    japanese_explanation_prompt += "   - Tables showing declensions where relevant\n"
    japanese_explanation_prompt += "4. ğŸ—£ï¸ **å®Ÿç”¨ãƒ•ãƒ¬ãƒ¼ã‚º** - Practical phrases related to the topic\n\n"
    japanese_explanation_prompt += "Format the article in markdown with proper headers, tables, and clear structure.\n"
    japanese_explanation_prompt += "Focus on A2-level grammar and vocabulary that Japanese learners need.\n"
    japanese_explanation_prompt += "Write everything in Japanese.\n"
    japanese_explanation_prompt += "Do not include practice questions, study tips, or homework sections."

    try:
        jp_message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            messages=[{"role": "user", "content": japanese_explanation_prompt}]
        )

        japanese_explanation = jp_message.content[0].text.strip()

        # æ—¥æœ¬èªè§£èª¬è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        jp_blog_content = "---\n"
        jp_blog_content += 'title: "' + unique_title + ' - æ—¥æœ¬èªè§£èª¬"\n'
        jp_blog_content += "date: " + today + "\n"
        jp_blog_content += 'topic: "' + selected_topic + '"\n'
        jp_blog_content += 'type: "japanese_explanation"\n'
        jp_blog_content += 'difficulty_level: "A2"\n'
        jp_blog_content += 'original_post: "' + today + '.md"\n'
        jp_blog_content += "---\n\n"
        jp_blog_content += "# ğŸ“š A2è§£èª¬: " + selected_topic + "\n\n"
        jp_blog_content += "**åŸæ–‡è¨˜äº‹**: [" + unique_title + "](" + today + ".md)\n\n"
        jp_blog_content += "---\n\n"
        jp_blog_content += japanese_explanation + "\n"

        jp_blog_file = blog_dir / (today + "-jp.md")
        with open(jp_blog_file, 'w', encoding='utf-8') as f:
            f.write(jp_blog_content)

        print("ğŸ“„ Japanese explanation created: " + str(jp_blog_file))

    except Exception as e:
        print(f"âŒ Error generating Japanese explanation: {e}")

    # è‹±èªè§£èª¬è¨˜äº‹ã®ç”Ÿæˆ
    print("ğŸ‡ºğŸ‡¸ Generating English explanation...")

    english_explanation_prompt = "Create a detailed English explanation article for A2 German learners based on this German text:\n\n"
    english_explanation_prompt += "GERMAN TEXT:\n"
    english_explanation_prompt += generated_text + "\n\n"
    english_explanation_prompt += "TOPIC: " + selected_topic + "\n"
    english_explanation_prompt += "NEW A2 WORDS: " + ', '.join(new_a2_words) + "\n\n"
    english_explanation_prompt += "Please create a comprehensive English explanation article that includes:\n\n"
    english_explanation_prompt += "1. ğŸ¯ **Today's Learning Goals** - Learning objectives for the day\n"
    english_explanation_prompt += "2. ğŸ“– **Key Vocabulary Analysis** - Detailed vocabulary explanation with:\n"
    english_explanation_prompt += "   - Category grouping (Family relationships, Professions, Character traits, Activities etc.)\n"
    english_explanation_prompt += "   - Pronunciation guide in IPA or simple phonetics\n"
    english_explanation_prompt += "   - Example sentences\n"
    english_explanation_prompt += "   - Grammar notes and word formation\n"
    english_explanation_prompt += "3. ğŸ“ **Important Grammar Points** - Important grammar points from the text with:\n"
    english_explanation_prompt += "   - Examples extracted from the original text\n"
    english_explanation_prompt += "   - Detailed explanations of case changes, verb conjugations\n"
    english_explanation_prompt += "   - Tables showing declensions where relevant\n"
    english_explanation_prompt += "4. ğŸ—£ï¸ **Useful Phrases** - Practical phrases related to the topic with translations\n\n"
    english_explanation_prompt += "Format the article in markdown with proper headers, tables, and clear structure.\n"
    english_explanation_prompt += "Focus on A2-level grammar and vocabulary explanations for English speakers learning German.\n"
    english_explanation_prompt += "Write everything in clear, educational English.\n"
    english_explanation_prompt += "Do not include practice exercises, study tips, or homework sections."

    try:
        en_message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            messages=[{"role": "user", "content": english_explanation_prompt}]
        )

        english_explanation = en_message.content[0].text.strip()

        # è‹±èªè§£èª¬è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        en_blog_content = "---\n"
        en_blog_content += 'title: "' + unique_title + ' - English Explanation"\n'
        en_blog_content += "date: " + today + "\n"
        en_blog_content += 'topic: "' + selected_topic + '"\n'
        en_blog_content += 'type: "english_explanation"\n'
        en_blog_content += 'difficulty_level: "A2"\n'
        en_blog_content += 'original_post: "' + today + '.md"\n'
        en_blog_content += "---\n\n"
        en_blog_content += "# ğŸ“š A2 German Study Guide: " + selected_topic + "\n\n"
        en_blog_content += "**Original Article**: [" + unique_title + "](" + today + ".md)\n\n"
        en_blog_content += "---\n\n"
        en_blog_content += english_explanation + "\n"

        en_blog_file = blog_dir / (today + "-en.md")
        with open(en_blog_file, 'w', encoding='utf-8') as f:
            f.write(en_blog_content)

        print("ğŸ“„ English explanation created: " + str(en_blog_file))

    except Exception as e:
        print(f"âŒ Error generating English explanation: {e}")
        
    print("ğŸ¯ Title: " + unique_title)
    print("ğŸ“ˆ New A2 words learned: " + str(len(new_a2_words)))
    print("ğŸ“š A2 Progress: " + str(daily_report['a2_progress']['words_learned']) + "/" + str(daily_report['a2_progress']['total_target']) + " (" + str(daily_report['a2_progress']['progress_percent']) + "%)")
    print("â±ï¸  Estimated completion: " + str(daily_report['a2_progress']['estimated_completion_days']) + " days")
    print("ğŸ‡¯ğŸ‡µ Japanese explanation: " + today + "-jp.md")
    print("ğŸ‡ºğŸ‡¸ English explanation: " + today + "-en.md")

    # A2é€²æ—ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´ææ¡ˆ
    if daily_report["a2_progress"]["estimated_completion_days"] > 150:
        print("âš ï¸  WARNING: A2 completion may take longer than 120 days! Increasing vocabulary focus.")
    elif daily_report["a2_progress"]["estimated_completion_days"] < 90:
        print("ğŸš€ EXCELLENT: On track to complete A2 vocabulary ahead of schedule!")
    else:
        print("âœ… GOOD: A2 vocabulary completion on target for ~120 days")

    # æ¯æ—¥ã®èªå½™ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    reports_dir = Path("data/reports")
    reports_dir.mkdir(parents=True, exist_ok=True)

    daily_report_file = reports_dir / (today + "-vocabulary-report.json")
    with open(daily_report_file, 'w', encoding='utf-8') as f:
        json.dump(daily_report, f, ensure_ascii=False, indent=2)

    print("ğŸ“‹ Daily vocabulary report saved: " + str(daily_report_file))

    # é€±æ¬¡A2é€²æ—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ—¥æ›œæ—¥ã®ã¿ï¼‰
    if datetime.now().weekday() == 6:  # Sunday
        print("ğŸ“Š Generating weekly A2 progress summary...")

        # éå»7æ—¥é–“ã®A2å­¦ç¿’çµ±è¨ˆ
        week_a2_stats = {
            "week_ending": today,
            "a2_words_learned_total": daily_report["a2_progress"]["words_learned"],
            "a2_progress_percent": daily_report["a2_progress"]["progress_percent"],
            "estimated_completion": daily_report["a2_progress"]["estimated_completion_days"],
            "daily_pace": daily_report["a2_progress"]["daily_pace"],
            "remaining_words": daily_report["a2_progress"]["remaining_words"]
        }

        weekly_summary = "---\n"
        weekly_summary += 'title: "[A2] WÃ¶chentlicher Fortschrittsbericht"\n'
        weekly_summary += "date: " + today + "\n"
        weekly_summary += 'type: "weekly_progress_report"\n'
        weekly_summary += "---\n\n"
        weekly_summary += "# ğŸ“Š WÃ¶chentlicher A2-Deutschlern-Fortschritt\n\n"
        weekly_summary += "## ğŸ¯ A2-Vokabular Fortschritt\n"
        weekly_summary += "- **Gelernte A2-WÃ¶rter**: " + str(week_a2_stats["a2_words_learned_total"]) + "/1.500 (" + str(week_a2_stats["a2_progress_percent"]) + "%)\n"
        weekly_summary += "- **Verbleibende WÃ¶rter**: " + str(week_a2_stats["remaining_words"]) + "\n"
        weekly_summary += "- **Aktuelles Lerntempo**: " + str(round(week_a2_stats["daily_pace"], 1)) + " neue A2-WÃ¶rter pro Tag\n"
        weekly_summary += "- **GeschÃ¤tzte Restzeit**: " + str(week_a2_stats["estimated_completion"]) + " Tage\n\n"
        weekly_summary += "## ğŸ“ˆ Leistungsbeurteilung\n"
        
        if week_a2_stats["estimated_completion"] < 90:
            weekly_summary += "ğŸš€ Ausgezeichnet! Vor dem Zeitplan.\n\n"
        elif week_a2_stats["estimated_completion"] <= 130:
            weekly_summary += "âœ… Gut! Im Zeitplan fÃ¼r ~120 Tage.\n\n"
        else:
            weekly_summary += "âš ï¸ Achtung: MÃ¶glicherweise lÃ¤nger als 120 Tage.\n\n"
            
        weekly_summary += "## ğŸª Empfehlungen\n"
        weekly_summary += "- Fokus auf alltÃ¤gliche Substantive und Verben\n"
        weekly_summary += "- Vermeidung bereits hÃ¤ufig verwendeter WÃ¶rter\n"
        weekly_summary += "- Gezieltes Lernen spezifischer A2-Themenbereiche\n"
        weekly_summary += "- RegelmÃ¤ÃŸige Wiederholung neuer Vokabeln\n"

        weekly_file = blog_dir / (today + "-weekly-a2-progress.md")
        with open(weekly_file, 'w', encoding='utf-8') as f:
            f.write(weekly_summary)
        print("ğŸ“‹ Weekly A2 progress report created: " + str(weekly_file))

if __name__ == "__main__":
    main()