#!/usr/bin/env python3
"""
German Vocabulary Blog Generator
Generates blog content and podcast audio from word lists
"""
import os
import sys
import re
import time
import json
from datetime import datetime
from pathlib import Path
import glob
import anthropic
from google.cloud import texttospeech
import xml.etree.ElementTree as ET
from xml.dom import minidom

# Check API keys
if 'ANTHROPIC_API_KEY' not in os.environ:
    print("ERROR: ANTHROPIC_API_KEY not found in environment variables")
    sys.exit(1)

def generate_jekyll_filename(episode_number, date_str):
    """Generate Jekyll-format filename for vocabulary episode"""
    return f"{date_str}-german-vocab-episode-{episode_number}.md"

def get_next_episode_number():
    """Get the next episode number by checking existing files"""
    posts_dir = Path("_posts")
    if not posts_dir.exists():
        return 1
    
    # Find all vocabulary episode files
    vocab_files = list(posts_dir.glob("*-german-vocab-episode-*.md"))
    if not vocab_files:
        return 1
    
    # Extract episode numbers
    episode_numbers = []
    for file in vocab_files:
        match = re.search(r'episode-(\d+)\.md$', file.name)
        if match:
            episode_numbers.append(int(match.group(1)))
    
    return max(episode_numbers) + 1 if episode_numbers else 1

def generate_etymology_and_memory(word, client):
    """Generate etymology and memory technique for a German word"""
    prompt = f"""For the German word "{word}", provide:

1. Japanese meaning (ç°¡æ½”ã«)
2. Etymology explanation in Japanese (èªæºã®èª¬æ˜)
3. Memory technique using Japanese phonetics (ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ã£ãŸè¦šãˆæ–¹)

Format your response as JSON with these exact keys:
- "meaning": Japanese meaning
- "etymology": Etymology in Japanese (2-3 sentences)
- "memory": Memory technique in Japanese using katakana sounds

Example format:
{{
    "meaning": "èµ°ã‚‹",
    "etymology": "å¤é«˜ãƒ‰ã‚¤ãƒ„èªã®ã€Œloufenã€ã‹ã‚‰æ´¾ç”Ÿã€‚å°æ¬§èªã®ã€Œ*leu-ã€ï¼ˆé€Ÿãå‹•ãï¼‰ãŒèªæºã€‚",
    "memory": "ã€Œãƒ©ã‚¦ãƒ•ã‚§ãƒ³ã€â†’ã€Œãƒ©ãƒ–ç·¨ã€â†’æ„›ï¼ˆãƒ©ãƒ–ï¼‰ã‚’ç·¨ã‚€ãŸã‚ã«èµ°ã‚‹"
}}

Respond ONLY with the JSON, no additional text."""

    for attempt in range(3):
        try:
            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                messages=[{"role": "user", "content": prompt}]
            )
            
            response_text = message.content[0].text.strip()
            # Parse JSON response
            return json.loads(response_text)
            
        except Exception as e:
            print(f"Error generating etymology for {word} (attempt {attempt + 1}): {e}")
            if attempt < 2:
                time.sleep(5)
    
    # Fallback if API fails
    return {
        "meaning": "ï¼ˆæ„å‘³ï¼‰",
        "etymology": "ï¼ˆèªæºæƒ…å ±ï¼‰",
        "memory": "ï¼ˆè¦šãˆæ–¹ï¼‰"
    }

def generate_example_sentences(word, client):
    """Generate 3 example sentences for a word"""
    prompt = f"""Generate 3 example sentences for the German word "{word}" at A2-B1 level.

Requirements:
- Each sentence should be practical and useful
- Include a natural Japanese translation
- Sentences should demonstrate different uses of the word

Format as JSON array:
[
    {{"german": "German sentence", "japanese": "Japanese translation"}},
    {{"german": "German sentence", "japanese": "Japanese translation"}},
    {{"german": "German sentence", "japanese": "Japanese translation"}}
]

Respond ONLY with the JSON array."""

    for attempt in range(3):
        try:
            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=800,
                messages=[{"role": "user", "content": prompt}]
            )
            
            response_text = message.content[0].text.strip()
            return json.loads(response_text)
            
        except Exception as e:
            print(f"Error generating sentences for {word} (attempt {attempt + 1}): {e}")
            if attempt < 2:
                time.sleep(5)
    
    # Fallback
    return [
        {"german": f"Beispiel mit {word}.", "japanese": "ä¾‹æ–‡"},
        {"german": f"Satz mit {word}.", "japanese": "æ–‡"},
        {"german": f"Text mit {word}.", "japanese": "ãƒ†ã‚­ã‚¹ãƒˆ"}
    ]

def format_vocabulary_content(word, word_data, sentences, index):
    """Format vocabulary content for blog post"""
    content = f"""## å˜èª{index}: {word}

**æ„å‘³**: {word_data['meaning']}
**èªæº**: {word_data['etymology']}
**è¦šãˆæ–¹**: {word_data['memory']}

**ã‚†ã£ãã‚Š**: {word}... {word}... {word}
**æ™®é€š**: {word}, {word}, {word}
**æ—©å£**: {word}-{word}-{word}

**ä¾‹æ–‡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**:
"""
    
    for i, sentence in enumerate(sentences, 1):
        content += f"{i}. {sentence['german']} ({sentence['japanese']})\n"
    
    content += f"""
**ã‚†ã£ãã‚Š**: {word}... {word}... {word}
**æ™®é€š**: {word}, {word}, {word}
**æ—©å£**: {word}-{word}-{word}

"""
    
    return content

def create_ssml_content(words_data):
    """Create SSML content for Google Cloud TTS"""
    root = ET.Element("speak")
    
    # Introduction
    intro_ja = ET.SubElement(root, "lang")
    intro_ja.set("xml:lang", "ja-JP")
    intro_ja.text = "ä»Šæ—¥ã®ãƒ‰ã‚¤ãƒ„èªå˜èªå­¦ç¿’ã‚’å§‹ã‚ã¾ã—ã‚‡ã†ã€‚"
    
    ET.SubElement(root, "break", time="1s")
    
    # Process each word
    for i, (word, data) in enumerate(words_data.items(), 1):
        # Word number announcement
        word_num = ET.SubElement(root, "lang")
        word_num.set("xml:lang", "ja-JP")
        word_num.text = f"å˜èª{i}ï¼š"
        
        # German word
        word_de = ET.SubElement(root, "lang")
        word_de.set("xml:lang", "de-DE")
        word_de.text = word
        
        ET.SubElement(root, "break", time="0.5s")
        
        # Meaning
        meaning = ET.SubElement(root, "lang")
        meaning.set("xml:lang", "ja-JP")
        meaning.text = f"æ„å‘³ï¼š{data['meaning']}"
        
        ET.SubElement(root, "break", time="1s")
        
        # Slow pronunciation
        slow_ja = ET.SubElement(root, "lang")
        slow_ja.set("xml:lang", "ja-JP")
        slow_ja.text = "ã‚†ã£ãã‚Šï¼š"
        
        for _ in range(3):
            slow = ET.SubElement(root, "prosody", rate="0.75")
            slow_de = ET.SubElement(slow, "lang")
            slow_de.set("xml:lang", "de-DE")
            slow_de.text = word
            ET.SubElement(root, "break", time="0.5s")
        
        # Normal speed
        normal_ja = ET.SubElement(root, "lang")
        normal_ja.set("xml:lang", "ja-JP")
        normal_ja.text = "æ™®é€šï¼š"
        
        for _ in range(3):
            normal_de = ET.SubElement(root, "lang")
            normal_de.set("xml:lang", "de-DE")
            normal_de.text = word
            ET.SubElement(root, "break", time="0.3s")
        
        # Fast speed
        fast_ja = ET.SubElement(root, "lang")
        fast_ja.set("xml:lang", "ja-JP")
        fast_ja.text = "æ—©å£ï¼š"
        
        fast = ET.SubElement(root, "prosody", rate="1.25")
        for j in range(3):
            fast_de = ET.SubElement(fast, "lang")
            fast_de.set("xml:lang", "de-DE")
            fast_de.text = word
            if j < 2:
                ET.SubElement(fast, "break", time="0.1s")
        
        ET.SubElement(root, "break", time="1s")
        
        # Example sentences
        examples_ja = ET.SubElement(root, "lang")
        examples_ja.set("xml:lang", "ja-JP")
        examples_ja.text = "ä¾‹æ–‡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š"
        
        ET.SubElement(root, "break", time="0.5s")
        
        for j, sentence in enumerate(data['sentences'], 1):
            # Sentence number
            num_ja = ET.SubElement(root, "lang")
            num_ja.set("xml:lang", "ja-JP")
            num_ja.text = f"{j}ç•ªã€‚"
            
            # German sentence
            sent_de = ET.SubElement(root, "lang")
            sent_de.set("xml:lang", "de-DE")
            sent_de.text = sentence['german']
            
            ET.SubElement(root, "break", time="1s")
            
            # Japanese translation
            trans_ja = ET.SubElement(root, "lang")
            trans_ja.set("xml:lang", "ja-JP")
            trans_ja.text = sentence['japanese']
            
            ET.SubElement(root, "break", time="1s")
        
        # Practice repetition
        practice_ja = ET.SubElement(root, "lang")
        practice_ja.set("xml:lang", "ja-JP")
        practice_ja.text = "ã‚‚ã†ä¸€åº¦ç·´ç¿’ã—ã¾ã—ã‚‡ã†ã€‚"
        
        ET.SubElement(root, "break", time="0.5s")
        
        # Repeat slow/normal/fast
        for speed_name, rate in [("ã‚†ã£ãã‚Š", "0.75"), ("æ™®é€š", "1.0"), ("æ—©å£", "1.25")]:
            speed_ja = ET.SubElement(root, "lang")
            speed_ja.set("xml:lang", "ja-JP")
            speed_ja.text = f"{speed_name}ï¼š"
            
            if rate != "1.0":
                prosody = ET.SubElement(root, "prosody", rate=rate)
                parent = prosody
            else:
                parent = root
            
            for _ in range(3):
                repeat_de = ET.SubElement(parent, "lang")
                repeat_de.set("xml:lang", "de-DE")
                repeat_de.text = word
                ET.SubElement(parent, "break", time="0.3s")
        
        ET.SubElement(root, "break", time="2s")
    
    # Quiz section
    quiz_ja = ET.SubElement(root, "lang")
    quiz_ja.set("xml:lang", "ja-JP")
    quiz_ja.text = "ãã‚Œã§ã¯ã€ã‚¯ã‚¤ã‚ºã‚¿ã‚¤ãƒ ã§ã™ã€‚æ¬¡ã®ãƒ‰ã‚¤ãƒ„èªã‚’èã„ã¦ã€æ„å‘³ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚"
    
    ET.SubElement(root, "break", time="1s")
    
    for i, (word, data) in enumerate(words_data.items(), 1):
        quiz_num = ET.SubElement(root, "lang")
        quiz_num.set("xml:lang", "ja-JP")
        quiz_num.text = f"å•é¡Œ{i}ï¼š"
        
        quiz_word = ET.SubElement(root, "lang")
        quiz_word.set("xml:lang", "de-DE")
        quiz_word.text = word
        
        # 3 second pause for answer
        ET.SubElement(root, "break", time="3s")
        
        answer = ET.SubElement(root, "lang")
        answer.set("xml:lang", "ja-JP")
        answer.text = f"ç­”ãˆã¯ã€{data['meaning']}ã§ã—ãŸã€‚"
        
        ET.SubElement(root, "break", time="1s")
    
    # Closing
    closing = ET.SubElement(root, "lang")
    closing.set("xml:lang", "ja-JP")
    closing.text = "ä»Šæ—¥ã®å˜èªå­¦ç¿’ã¯ã“ã‚Œã§çµ‚ã‚ã‚Šã§ã™ã€‚å¾©ç¿’ã‚’å¿˜ã‚Œãšã«ã€é ‘å¼µã£ã¦ãã ã•ã„ï¼"
    
    return root

def prettify_xml(elem):
    """Return a pretty-printed XML string"""
    rough_string = ET.tostring(elem, encoding='unicode')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ")

def generate_audio_file(ssml_content, output_path):
    """Generate MP3 audio file using Google Cloud TTS"""
    client = texttospeech.TextToSpeechClient()
    
    # Convert SSML to string
    ssml_text = ET.tostring(ssml_content, encoding='unicode')
    
    synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)
    
    # Voice configuration (using a female German voice as primary)
    voice = texttospeech.VoiceSelectionParams(
        language_code="de-DE",
        name="de-DE-Wavenet-F",  # High quality female voice
        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
    )
    
    # Audio configuration
    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3,
        speaking_rate=1.0,
        pitch=0.0
    )
    
    # Generate audio
    try:
        response = client.synthesize_speech(
            input=synthesis_input,
            voice=voice,
            audio_config=audio_config
        )
        
        # Save audio file
        with open(output_path, 'wb') as out:
            out.write(response.audio_content)
        
        print(f"âœ… Audio saved: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ Error generating audio: {e}")
        return False

def process_word_file(word_file_path, episode_number):
    """Process a word file and generate all content"""
    print(f"ğŸ“„ Processing word file: {word_file_path}")
    
    # Read words
    with open(word_file_path, 'r', encoding='utf-8') as f:
        words = [line.strip() for line in f if line.strip()]
    
    if not words:
        print("âŒ No words found in file")
        return False
    
    print(f"ğŸ“ Found {len(words)} words to process")
    
    # Initialize Anthropic client
    client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])
    
    # Generate data for each word
    words_data = {}
    
    for i, word in enumerate(words, 1):
        print(f"ğŸ”„ Processing word {i}/{len(words)}: {word}")
        
        # Generate etymology and memory technique
        word_info = generate_etymology_and_memory(word, client)
        
        # Generate example sentences
        sentences = generate_example_sentences(word, client)
        
        words_data[word] = {
            'meaning': word_info['meaning'],
            'etymology': word_info['etymology'],
            'memory': word_info['memory'],
            'sentences': sentences
        }
        
        # Small delay to avoid rate limits
        if i < len(words):
            time.sleep(2)
    
    # Generate blog content
    today = datetime.now().strftime('%Y-%m-%d')
    blog_content = f"""---
layout: post
title: "German Vocabulary Episode {episode_number}"
date: {today}
categories: [german, vocabulary, learning]
episode_number: {episode_number}
audio_file: "episode-{episode_number}.mp3"
---

# ãƒ‰ã‚¤ãƒ„èªå˜èªå­¦ç¿’ Episode {episode_number}

ä»Šæ—¥ã¯{len(words)}å€‹ã®æ–°ã—ã„å˜èªã‚’å­¦ç¿’ã—ã¾ã™ã€‚å„å˜èªã«ã¯èªæºèª¬æ˜ã€è¦šãˆæ–¹ã€ä¾‹æ–‡ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

"""
    
    # Add vocabulary content
    for i, (word, data) in enumerate(words_data.items(), 1):
        sentences = data['sentences']
        blog_content += format_vocabulary_content(word, data, sentences, i)
    
    # Add quiz section
    blog_content += """## ã‚¯ã‚¤ã‚º

ä»¥ä¸‹ã®å˜èªã®æ„å‘³ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ï¼š

"""
    
    for i, word in enumerate(words, 1):
        blog_content += f"{i}. **{word}** â†’ ï¼Ÿï¼Ÿï¼Ÿ\n"
    
    blog_content += "\n### ç­”ãˆ\n\n"
    
    for i, (word, data) in enumerate(words_data.items(), 1):
        blog_content += f"{i}. **{word}** â†’ {data['meaning']}\n"
    
    blog_content += f"""

---

ğŸ§ **Audio Version**: [Listen to Episode {episode_number}](/assets/audio/episode-{episode_number}.mp3)

ğŸ“š **Source**: Generated from word file on {today}
"""
    
    # Save blog post
    posts_dir = Path("_posts")
    posts_dir.mkdir(exist_ok=True)
    
    blog_filename = generate_jekyll_filename(episode_number, today)
    blog_path = posts_dir / blog_filename
    
    with open(blog_path, 'w', encoding='utf-8') as f:
        f.write(blog_content)
    
    print(f"âœ… Blog post created: {blog_path}")
    
    # Generate SSML content
    ssml_content = create_ssml_content(words_data)
    
    # Save SSML for debugging
    ssml_dir = Path("data/ssml")
    ssml_dir.mkdir(parents=True, exist_ok=True)
    
    ssml_path = ssml_dir / f"episode-{episode_number}-ssml.xml"
    with open(ssml_path, 'w', encoding='utf-8') as f:
        f.write(prettify_xml(ssml_content))
    
    print(f"âœ… SSML saved: {ssml_path}")
    
    # Generate audio file (only if Google Cloud credentials are available)
    if 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ:
        audio_dir = Path("assets/audio")
        audio_dir.mkdir(parents=True, exist_ok=True)
        
        audio_path = audio_dir / f"episode-{episode_number}.mp3"
        if generate_audio_file(ssml_content, audio_path):
            print(f"âœ… Audio generated: {audio_path}")
        else:
            print("âš ï¸  Audio generation failed")
    else:
        print("â„¹ï¸  Google Cloud credentials not found, skipping audio generation")
    
    return True

def main():
    """Main function to process latest word file"""
    print("ğŸš€ Starting German vocabulary content generation...")
    
    # Find the latest word file
    word_files = glob.glob("data/words/*.txt")
    
    if not word_files:
        print("âŒ No word files found in data/words/")
        sys.exit(1)
    
    # Get the most recently modified file
    latest_file = max(word_files, key=os.path.getmtime)
    
    # Get next episode number
    episode_number = get_next_episode_number()
    print(f"ğŸ“º Next episode number: {episode_number}")
    
    # Process the file
    if process_word_file(latest_file, episode_number):
        print("\nâœ… Vocabulary content generation completed!")
        print(f"ğŸ“š Episode: {episode_number}")
        print(f"ğŸ“„ Word file: {latest_file}")
    else:
        print("\nâŒ Content generation failed")
        sys.exit(1)

if __name__ == "__main__":
    main()